


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tllib.alignment.d_adapt.modeling.meta_arch.retinanet &mdash; Transfer Learning Library 0.0.24 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" /> 

  
  <script src="../../../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
<!--      pytorch logo -->
<!--      <a class="header-logo" href="transfer.thuml.ai" aria-label="PyTorch"></a>-->

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="">Full Survey</a>
          </li>
          <li>
            <a href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">Paper List</a>
          </li> -->
          <li>
            <li>
              <a href="transfer.thuml.ai">Docs</a>
            </li>
            
            <li>
              <a href="https://github.com/thuml/Transfer-Learning-Library/">Github</a>
            </li>

            <li>
              <a href="https://arxiv.org/abs/2201.05867">Survey</a>
            </li>

          <li>
            <a href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">Paper List</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Transfer Learning API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/alignment/index.html">Feature Alignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/translation.html">Domain Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/self_training.html">Self Training Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/reweight.html">Re-weighting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/normalization.html">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/regularization.html">Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/ranking.html">Ranking</a></li>
</ul>
<p class="caption"><span class="caption-text">Common API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/vision/index.html">Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../tllib/utils/index.html">Utilities</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../../../index.html">Module code</a> &gt;</li>
        
      <li>tllib.alignment.d_adapt.modeling.meta_arch.retinanet</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for tllib.alignment.d_adapt.modeling.meta_arch.retinanet</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">@author: Junguang Jiang</span>
<span class="sd">@contact: JiangJunguang1123@outlook.com</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">detectron2.structures</span> <span class="kn">import</span> <span class="n">BoxMode</span><span class="p">,</span> <span class="n">Boxes</span><span class="p">,</span> <span class="n">Instances</span><span class="p">,</span> <span class="n">pairwise_iou</span><span class="p">,</span> <span class="n">ImageList</span>
<span class="kn">from</span> <span class="nn">detectron2.layers</span> <span class="kn">import</span> <span class="n">ShapeSpec</span><span class="p">,</span> <span class="n">batched_nms</span><span class="p">,</span> <span class="n">cat</span><span class="p">,</span> <span class="n">get_norm</span><span class="p">,</span> <span class="n">nonzero_tuple</span>
<span class="kn">from</span> <span class="nn">detectron2.modeling</span> <span class="kn">import</span> <span class="n">detector_postprocess</span>
<span class="kn">from</span> <span class="nn">detectron2.modeling.meta_arch.build</span> <span class="kn">import</span> <span class="n">META_ARCH_REGISTRY</span>
<span class="kn">from</span> <span class="nn">detectron2.data.detection_utils</span> <span class="kn">import</span> <span class="n">convert_image_to_rgb</span>
<span class="kn">from</span> <span class="nn">detectron2.utils.events</span> <span class="kn">import</span> <span class="n">get_event_storage</span>

<span class="kn">from</span> <span class="nn">tllib.vision.models.object_detection.meta_arch</span> <span class="kn">import</span> <span class="n">TLRetinaNet</span>
<span class="kn">from</span> <span class="nn">..matcher</span> <span class="kn">import</span> <span class="n">MaxOverlapMatcher</span>


<div class="viewcode-block" id="DecoupledRetinaNet"><a class="viewcode-back" href="../../../../../../tllib/alignment/domain_adversarial.html#tllib.alignment.d_adapt.modeling.meta_arch.DecoupledRetinaNet">[docs]</a><span class="nd">@META_ARCH_REGISTRY</span><span class="o">.</span><span class="n">register</span><span class="p">()</span>
<span class="k">class</span> <span class="nc">DecoupledRetinaNet</span><span class="p">(</span><span class="n">TLRetinaNet</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    RetinaNet for Decoupled Adaptation (D-adapt).</span>

<span class="sd">    Different from that in Supervised Learning, DecoupledRetinaNet</span>
<span class="sd">    1. accepts unlabeled images and uses the feedbacks from adaptors as supervision during training</span>
<span class="sd">    2. generate foreground and background proposals during inference</span>

<span class="sd">    Args:</span>
<span class="sd">        backbone: a backbone module, must follow detectron2&#39;s backbone interface</span>
<span class="sd">        head (nn.Module): a module that predicts logits and regression deltas</span>
<span class="sd">            for each level from a list of per-level features</span>
<span class="sd">        head_in_features (Tuple[str]): Names of the input feature maps to be used in head</span>
<span class="sd">        anchor_generator (nn.Module): a module that creates anchors from a</span>
<span class="sd">            list of features. Usually an instance of :class:`AnchorGenerator`</span>
<span class="sd">        box2box_transform (Box2BoxTransform): defines the transform from anchors boxes to</span>
<span class="sd">            instance boxes</span>
<span class="sd">        anchor_matcher (Matcher): label the anchors by matching them with ground truth.</span>
<span class="sd">        num_classes (int): number of classes. Used to label background proposals.</span>

<span class="sd">        # Loss parameters:</span>
<span class="sd">        focal_loss_alpha (float): focal_loss_alpha</span>
<span class="sd">        focal_loss_gamma (float): focal_loss_gamma</span>
<span class="sd">        smooth_l1_beta (float): smooth_l1_beta</span>
<span class="sd">        box_reg_loss_type (str): Options are &quot;smooth_l1&quot;, &quot;giou&quot;</span>

<span class="sd">        # Inference parameters:</span>
<span class="sd">        test_score_thresh (float): Inference cls score threshold, only anchors with</span>
<span class="sd">            score &gt; INFERENCE_TH are considered for inference (to improve speed)</span>
<span class="sd">        test_topk_candidates (int): Select topk candidates before NMS</span>
<span class="sd">        test_nms_thresh (float): Overlap threshold used for non-maximum suppression</span>
<span class="sd">            (suppress boxes with IoU &gt;= this threshold)</span>
<span class="sd">        max_detections_per_image (int):</span>
<span class="sd">            Maximum number of detections to return per image during inference</span>
<span class="sd">            (100 is based on the limit established for the COCO dataset).</span>

<span class="sd">        # Input parameters</span>
<span class="sd">        pixel_mean (Tuple[float]):</span>
<span class="sd">            Values to be used for image normalization (BGR order).</span>
<span class="sd">            To train on images of different number of channels, set different mean &amp; std.</span>
<span class="sd">            Default values are the mean pixel value from ImageNet: [103.53, 116.28, 123.675]</span>
<span class="sd">        pixel_std (Tuple[float]):</span>
<span class="sd">            When using pre-trained models in Detectron1 or any MSRA models,</span>
<span class="sd">            std has been absorbed into its conv1 weights, so the std needs to be set 1.</span>
<span class="sd">            Otherwise, you can use [57.375, 57.120, 58.395] (ImageNet std)</span>
<span class="sd">        vis_period (int):</span>
<span class="sd">            The period (in terms of steps) for minibatch visualization at train time.</span>
<span class="sd">            Set to 0 to disable.</span>
<span class="sd">        input_format (str): Whether the model needs RGB, YUV, HSV etc.</span>
<span class="sd">        finetune (bool): whether finetune the detector or train from scratch. Default: True</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - batched_inputs: a list, batched outputs of :class:`DatasetMapper`.</span>
<span class="sd">          Each item in the list contains the inputs for one image.</span>
<span class="sd">          For now, each item in the list is a dict that contains:</span>
<span class="sd">            * image: Tensor, image in (C, H, W) format.</span>
<span class="sd">            * instances (optional): groundtruth :class:`Instances`</span>
<span class="sd">            * &quot;height&quot;, &quot;width&quot; (int): the output resolution of the model, used in inference.</span>
<span class="sd">              See :meth:`postprocess` for details.</span>
<span class="sd">        - labeled (bool, optional): whether has ground-truth label</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - outputs: A list of dict where each dict is the output for one input image.</span>
<span class="sd">          The dict contains a key &quot;instances&quot; whose value is a :class:`Instances`</span>
<span class="sd">          and a key &quot;features&quot; whose value is the features of middle layers.</span>
<span class="sd">          The :class:`Instances` object has the following keys:</span>
<span class="sd">          &quot;pred_boxes&quot;, &quot;pred_classes&quot;, &quot;scores&quot;, &quot;pred_masks&quot;, &quot;pred_keypoints&quot;</span>
<span class="sd">        - losses: A dict of different losses</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="n">max_samples_per_level</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoupledRetinaNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_samples_per_level</span> <span class="o">=</span> <span class="n">max_samples_per_level</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_matcher</span> <span class="o">=</span> <span class="n">MaxOverlapMatcher</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward_training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">gt_instances</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">feedbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">labeled</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="c1"># Transpose the Hi*Wi*A dimension to the middle:</span>
        <span class="n">pred_logits</span><span class="p">,</span> <span class="n">pred_anchor_deltas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_dense_predictions</span><span class="p">(</span>
            <span class="n">predictions</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">anchors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">anchor_generator</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">labeled</span><span class="p">:</span>
            <span class="n">gt_labels</span><span class="p">,</span> <span class="n">gt_boxes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_anchors</span><span class="p">(</span><span class="n">anchors</span><span class="p">,</span> <span class="n">gt_instances</span><span class="p">)</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">(</span><span class="n">anchors</span><span class="p">,</span> <span class="n">pred_logits</span><span class="p">,</span> <span class="n">gt_labels</span><span class="p">,</span> <span class="n">pred_anchor_deltas</span><span class="p">,</span> <span class="n">gt_boxes</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">proposal_labels</span><span class="p">,</span> <span class="n">proposal_boxes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_pseudo_anchors</span><span class="p">(</span><span class="n">anchors</span><span class="p">,</span> <span class="n">feedbacks</span><span class="p">)</span>
            <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">(</span><span class="n">anchors</span><span class="p">,</span> <span class="n">pred_logits</span><span class="p">,</span> <span class="n">proposal_labels</span><span class="p">,</span> <span class="n">pred_anchor_deltas</span><span class="p">,</span> <span class="n">proposal_boxes</span><span class="p">)</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;loss_box_reg&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">losses</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batched_inputs</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]],</span> <span class="n">labeled</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">images</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">preprocess_image</span><span class="p">(</span><span class="n">batched_inputs</span><span class="p">)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="n">features</span><span class="p">[</span><span class="n">f</span><span class="p">]</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_in_features</span><span class="p">]</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;instances&quot;</span> <span class="ow">in</span> <span class="n">batched_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">gt_instances</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;instances&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched_inputs</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">gt_instances</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="s2">&quot;feedbacks&quot;</span> <span class="ow">in</span> <span class="n">batched_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">feedbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;feedbacks&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">batched_inputs</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">feedbacks</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_training</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">gt_instances</span><span class="p">,</span> <span class="n">feedbacks</span><span class="p">,</span> <span class="n">labeled</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vis_period</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">storage</span> <span class="o">=</span> <span class="n">get_event_storage</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">storage</span><span class="o">.</span><span class="n">iter</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">vis_period</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_inference</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">visualize_training</span><span class="p">(</span><span class="n">batched_inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">feedbacks</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">losses</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># sample_background must be called before inference</span>
            <span class="c1"># since inference will change predictions</span>
            <span class="n">background_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_background</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
            <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_inference</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

            <span class="n">processed_results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">results_per_image</span><span class="p">,</span> <span class="n">background_results_per_image</span><span class="p">,</span> <span class="n">input_per_image</span><span class="p">,</span> <span class="n">image_size</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
                    <span class="n">results</span><span class="p">,</span> <span class="n">background_results</span><span class="p">,</span> <span class="n">batched_inputs</span><span class="p">,</span> <span class="n">images</span><span class="o">.</span><span class="n">image_sizes</span>
            <span class="p">):</span>
                <span class="n">height</span> <span class="o">=</span> <span class="n">input_per_image</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="n">image_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">width</span> <span class="o">=</span> <span class="n">input_per_image</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;width&quot;</span><span class="p">,</span> <span class="n">image_size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">r</span> <span class="o">=</span> <span class="n">detector_postprocess</span><span class="p">(</span><span class="n">results_per_image</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
                <span class="n">background_r</span> <span class="o">=</span> <span class="n">detector_postprocess</span><span class="p">(</span><span class="n">background_results_per_image</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
                <span class="n">processed_results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;instances&quot;</span><span class="p">:</span> <span class="n">r</span><span class="p">,</span> <span class="s2">&quot;background&quot;</span><span class="p">:</span> <span class="n">background_r</span><span class="p">})</span>
            <span class="k">return</span> <span class="n">processed_results</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">label_pseudo_anchors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">anchors</span><span class="p">,</span> <span class="n">instances</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            anchors (list[Boxes]): A list of #feature level Boxes.</span>
<span class="sd">                The Boxes contains anchors of this image on the specific feature level.</span>
<span class="sd">            instances (list[Instances]): a list of N `Instances`s. The i-th</span>
<span class="sd">                `Instances` contains the ground-truth per-instance annotations</span>
<span class="sd">                for the i-th input image.</span>

<span class="sd">        Returns:</span>
<span class="sd">            list[Tensor]:</span>
<span class="sd">                List of #img tensors. i-th element is a vector of labels whose length is</span>
<span class="sd">                the total number of anchors across all feature maps (sum(Hi * Wi * A)).</span>
<span class="sd">                Label values are in {-1, 0, ..., K}, with -1 means ignore, and K means background.</span>
<span class="sd">            list[Tensor]:</span>
<span class="sd">                i-th element is a Rx4 tensor, where R is the total number of anchors across</span>
<span class="sd">                feature maps. The values are the matched gt boxes for each anchor.</span>
<span class="sd">                Values are undefined for those anchors not labeled as foreground.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">anchors</span> <span class="o">=</span> <span class="n">Boxes</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">anchors</span><span class="p">)</span>  <span class="c1"># Rx4</span>

        <span class="n">gt_labels</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">matched_gt_boxes</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">gt_per_image</span> <span class="ow">in</span> <span class="n">instances</span><span class="p">:</span>
            <span class="n">match_quality_matrix</span> <span class="o">=</span> <span class="n">pairwise_iou</span><span class="p">(</span><span class="n">gt_per_image</span><span class="o">.</span><span class="n">gt_boxes</span><span class="p">,</span> <span class="n">anchors</span><span class="p">)</span>
            <span class="n">matched_idxs</span><span class="p">,</span> <span class="n">anchor_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_matcher</span><span class="p">(</span><span class="n">match_quality_matrix</span><span class="p">)</span>
            <span class="k">del</span> <span class="n">match_quality_matrix</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gt_per_image</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">matched_gt_boxes_i</span> <span class="o">=</span> <span class="n">gt_per_image</span><span class="o">.</span><span class="n">gt_boxes</span><span class="o">.</span><span class="n">tensor</span><span class="p">[</span><span class="n">matched_idxs</span><span class="p">]</span>

                <span class="n">gt_labels_i</span> <span class="o">=</span> <span class="n">gt_per_image</span><span class="o">.</span><span class="n">gt_classes</span><span class="p">[</span><span class="n">matched_idxs</span><span class="p">]</span>
                <span class="c1"># Anchors with label -1 are ignored.</span>
                <span class="n">gt_labels_i</span><span class="p">[</span><span class="n">anchor_labels</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">matched_gt_boxes_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">anchors</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
                <span class="n">gt_labels_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">matched_idxs</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span>

            <span class="n">gt_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gt_labels_i</span><span class="p">)</span>
            <span class="n">matched_gt_boxes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">matched_gt_boxes_i</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">gt_labels</span><span class="p">,</span> <span class="n">matched_gt_boxes</span>

    <span class="k">def</span> <span class="nf">sample_background</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">:</span> <span class="n">ImageList</span><span class="p">,</span> <span class="n">features</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">predictions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">):</span>
        <span class="n">pred_logits</span><span class="p">,</span> <span class="n">pred_anchor_deltas</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_transpose_dense_predictions</span><span class="p">(</span>
            <span class="n">predictions</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">anchors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">anchor_generator</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

        <span class="n">results</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Instances</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">img_idx</span><span class="p">,</span> <span class="n">image_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">images</span><span class="o">.</span><span class="n">image_sizes</span><span class="p">):</span>
            <span class="n">scores_per_image</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">img_idx</span><span class="p">]</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pred_logits</span><span class="p">]</span>
            <span class="n">deltas_per_image</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="n">img_idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">pred_anchor_deltas</span><span class="p">]</span>
            <span class="n">results_per_image</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_background_single_image</span><span class="p">(</span>
                <span class="n">anchors</span><span class="p">,</span> <span class="n">scores_per_image</span><span class="p">,</span> <span class="n">deltas_per_image</span><span class="p">,</span> <span class="n">image_size</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">results_per_image</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">results</span>

    <span class="k">def</span> <span class="nf">sample_background_single_image</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">anchors</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Boxes</span><span class="p">],</span>
            <span class="n">box_cls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
            <span class="n">box_delta</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
            <span class="n">image_size</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="n">boxes_all</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">scores_all</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Iterate over every feature level</span>
        <span class="k">for</span> <span class="n">box_cls_i</span><span class="p">,</span> <span class="n">box_reg_i</span><span class="p">,</span> <span class="n">anchors_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">box_cls</span><span class="p">,</span> <span class="n">box_delta</span><span class="p">,</span> <span class="n">anchors</span><span class="p">):</span>
            <span class="c1"># (HxWxAxK,)</span>
            <span class="n">predicted_prob</span> <span class="o">=</span> <span class="n">box_cls_i</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">values</span>

            <span class="c1"># 1. Keep boxes with confidence score lower than threshold</span>
            <span class="n">keep_idxs</span> <span class="o">=</span> <span class="n">predicted_prob</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_score_thresh</span>
            <span class="n">anchor_idxs</span> <span class="o">=</span> <span class="n">nonzero_tuple</span><span class="p">(</span><span class="n">keep_idxs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

            <span class="c1"># 2. Random sample boxes</span>
            <span class="n">anchor_idxs</span> <span class="o">=</span> <span class="n">anchor_idxs</span><span class="p">[</span>
                <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">anchor_idxs</span><span class="p">)),</span> <span class="n">k</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">anchor_idxs</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_samples_per_level</span><span class="p">))]</span>
            <span class="n">predicted_prob</span> <span class="o">=</span> <span class="n">predicted_prob</span><span class="p">[</span><span class="n">anchor_idxs</span><span class="p">]</span>
            <span class="n">anchors_i</span> <span class="o">=</span> <span class="n">anchors_i</span><span class="p">[</span><span class="n">anchor_idxs</span><span class="p">]</span>

            <span class="n">boxes_all</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">anchors_i</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span>
            <span class="n">scores_all</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predicted_prob</span><span class="p">)</span>

        <span class="n">boxes_all</span><span class="p">,</span> <span class="n">scores_all</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">cat</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="n">boxes_all</span><span class="p">,</span> <span class="n">scores_all</span><span class="p">]</span>
        <span class="p">]</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">Instances</span><span class="p">(</span><span class="n">image_size</span><span class="p">)</span>
        <span class="n">result</span><span class="o">.</span><span class="n">pred_boxes</span> <span class="o">=</span> <span class="n">Boxes</span><span class="p">(</span><span class="n">boxes_all</span><span class="p">)</span>
        <span class="n">result</span><span class="o">.</span><span class="n">scores</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">scores_all</span>  <span class="c1"># the confidence score to be background</span>
        <span class="n">result</span><span class="o">.</span><span class="n">pred_classes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scores_all</span><span class="p">))])</span>
        <span class="k">return</span> <span class="n">result</span>

    <span class="k">def</span> <span class="nf">visualize_training</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batched_inputs</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">feedbacks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A function used to visualize ground truth images and final network predictions.</span>
<span class="sd">        It shows ground truth bounding boxes on the original image and up to 20</span>
<span class="sd">        predicted object bounding boxes on the original image.</span>

<span class="sd">        Args:</span>
<span class="sd">            batched_inputs (list): a list that contains input to the model.</span>
<span class="sd">            results (List[Instances]): a list of #images elements returned by forward_inference().</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">detectron2.utils.visualizer</span> <span class="kn">import</span> <span class="n">Visualizer</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">batched_inputs</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">results</span>
        <span class="p">),</span> <span class="s2">&quot;Cannot visualize inputs and results of different sizes&quot;</span>
        <span class="n">storage</span> <span class="o">=</span> <span class="n">get_event_storage</span><span class="p">()</span>
        <span class="n">max_boxes</span> <span class="o">=</span> <span class="mi">20</span>

        <span class="n">image_index</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># only visualize a single image</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">batched_inputs</span><span class="p">[</span><span class="n">image_index</span><span class="p">][</span><span class="s2">&quot;image&quot;</span><span class="p">]</span>
        <span class="n">img</span> <span class="o">=</span> <span class="n">convert_image_to_rgb</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_format</span><span class="p">)</span>
        <span class="n">v_gt</span> <span class="o">=</span> <span class="n">Visualizer</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">v_gt</span> <span class="o">=</span> <span class="n">v_gt</span><span class="o">.</span><span class="n">overlay_instances</span><span class="p">(</span><span class="n">boxes</span><span class="o">=</span><span class="n">batched_inputs</span><span class="p">[</span><span class="n">image_index</span><span class="p">][</span><span class="s2">&quot;instances&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">gt_boxes</span><span class="p">)</span>
        <span class="n">anno_img</span> <span class="o">=</span> <span class="n">v_gt</span><span class="o">.</span><span class="n">get_image</span><span class="p">()</span>
        <span class="n">processed_results</span> <span class="o">=</span> <span class="n">detector_postprocess</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="n">image_index</span><span class="p">],</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">predicted_boxes</span> <span class="o">=</span> <span class="n">processed_results</span><span class="o">.</span><span class="n">pred_boxes</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="n">v_pred</span> <span class="o">=</span> <span class="n">Visualizer</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">v_pred</span> <span class="o">=</span> <span class="n">v_pred</span><span class="o">.</span><span class="n">overlay_instances</span><span class="p">(</span><span class="n">boxes</span><span class="o">=</span><span class="n">predicted_boxes</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">max_boxes</span><span class="p">])</span>
        <span class="n">prop_img</span> <span class="o">=</span> <span class="n">v_pred</span><span class="o">.</span><span class="n">get_image</span><span class="p">()</span>

        <span class="n">num_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span>
        <span class="k">if</span> <span class="n">feedbacks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">v_feedback_gt</span> <span class="o">=</span> <span class="n">Visualizer</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">instance</span> <span class="o">=</span> <span class="n">feedbacks</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>
            <span class="n">v_feedback_gt</span> <span class="o">=</span> <span class="n">v_feedback_gt</span><span class="o">.</span><span class="n">overlay_instances</span><span class="p">(</span>
                <span class="n">boxes</span><span class="o">=</span><span class="n">instance</span><span class="o">.</span><span class="n">proposal_boxes</span><span class="p">[</span><span class="n">instance</span><span class="o">.</span><span class="n">gt_classes</span> <span class="o">!=</span> <span class="n">num_classes</span><span class="p">])</span>
            <span class="n">feedback_gt_img</span> <span class="o">=</span> <span class="n">v_feedback_gt</span><span class="o">.</span><span class="n">get_image</span><span class="p">()</span>

            <span class="n">v_feedback_gf</span> <span class="o">=</span> <span class="n">Visualizer</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">v_feedback_gf</span> <span class="o">=</span> <span class="n">v_feedback_gf</span><span class="o">.</span><span class="n">overlay_instances</span><span class="p">(</span>
                <span class="n">boxes</span><span class="o">=</span><span class="n">instance</span><span class="o">.</span><span class="n">proposal_boxes</span><span class="p">[</span><span class="n">instance</span><span class="o">.</span><span class="n">gt_classes</span> <span class="o">==</span> <span class="n">num_classes</span><span class="p">])</span>
            <span class="n">feedback_gf_img</span> <span class="o">=</span> <span class="n">v_feedback_gf</span><span class="o">.</span><span class="n">get_image</span><span class="p">()</span>

            <span class="n">vis_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">anno_img</span><span class="p">,</span> <span class="n">prop_img</span><span class="p">,</span> <span class="n">feedback_gt_img</span><span class="p">,</span> <span class="n">feedback_gf_img</span><span class="p">))</span>
            <span class="n">vis_img</span> <span class="o">=</span> <span class="n">vis_img</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">vis_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;Top: GT; Middle: Pred; Bottom: Feedback GT, Feedback GF&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">vis_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">anno_img</span><span class="p">,</span> <span class="n">prop_img</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">vis_img</span> <span class="o">=</span> <span class="n">vis_img</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">vis_name</span> <span class="o">=</span> <span class="s2">&quot;Left: GT bounding boxes;  Right: Predicted proposals&quot;</span>

        <span class="n">storage</span><span class="o">.</span><span class="n">put_image</span><span class="p">(</span><span class="n">vis_name</span><span class="p">,</span> <span class="n">vis_img</span><span class="p">)</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright THUML Group.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://github.com/pytorch/pytorch_sphinx_theme">PyTorch Sphinx Theme</a>.
      </div>
      <!-- <div>
        <a href="https://beian.miit.gov.cn/">京ICP备16023543号-1</a>
      </div> -->
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../../../" src="../../../../../../_static/documentation_options.js"></script>
         <script src="../../../../../../_static/jquery.js"></script>
         <script src="../../../../../../_static/underscore.js"></script>
         <script src="../../../../../../_static/doctools.js"></script>
         <script src="../../../../../../_static/language_data.js"></script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../../../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!-- <div class="col-md-4 text-center">
          <h2>Survey</h2>
          <p>Access comprehensive survey for transfer learning</p>
          <a class="with-right-arrow" href="">View Survey</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Papers</h2>
          <p>Access comprehensive paper list for transfer learning</p>
          <a class="with-right-arrow" href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">View Paper List</a>
        </div> -->

        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive documentation for Transfer Learning Library</p>
          <a class="with-right-arrow" href="transfer.thuml.ai">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get started for Transfer Learning Library</p>
          <a class="with-right-arrow" href="http://microhhh.com/get_started/installing.html">Get Started</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Paper List</h2>
          <p>Get started for transfer learning</p>
          <a class="with-right-arrow" href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">View Resources</a>
        </div>
      </div>
    </div>
  </div>



  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="transfer.thuml.ai" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="http://microhhh.com/get_started/installing.html">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="transfer.thuml.ai">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/thuml/Transfer-Learning-Library/">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>