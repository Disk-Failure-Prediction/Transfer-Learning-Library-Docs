


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tllib.alignment.mdd &mdash; Transfer Learning Library 0.0.24 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
<!--      pytorch logo -->
<!--      <a class="header-logo" href="transfer.thuml.ai" aria-label="PyTorch"></a>-->

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="">Full Survey</a>
          </li>
          <li>
            <a href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">Paper List</a>
          </li> -->
          <li>
            <li>
              <a href="transfer.thuml.ai">Docs</a>
            </li>
            
            <li>
              <a href="https://github.com/thuml/Transfer-Learning-Library/">Github</a>
            </li>

            <li>
              <a href="https://arxiv.org/abs/2201.05867">Survey</a>
            </li>

          <li>
            <a href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">Paper List</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Transfer Learning API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/alignment/index.html">Feature Alignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/translation.html">Domain Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/self_training.html">Self Training Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/reweight.html">Re-weighting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/normalization.html">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/regularization.html">Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/ranking.html">Ranking</a></li>
</ul>
<p class="caption"><span class="caption-text">Common API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/vision/index.html">Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tllib/utils/index.html">Utilities</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
      <li>tllib.alignment.mdd</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for tllib.alignment.mdd</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">@author: Junguang Jiang</span>
<span class="sd">@contact: JiangJunguang1123@outlook.com</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Callable</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">tllib.modules.grl</span> <span class="kn">import</span> <span class="n">WarmStartGradientReverseLayer</span>


<div class="viewcode-block" id="MarginDisparityDiscrepancy"><a class="viewcode-back" href="../../../tllib/alignment/hypothesis_adversarial.html#tllib.alignment.mdd.MarginDisparityDiscrepancy">[docs]</a><span class="k">class</span> <span class="nc">MarginDisparityDiscrepancy</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The margin disparity discrepancy (MDD) proposed in `Bridging Theory and Algorithm for Domain Adaptation (ICML 2019) &lt;https://arxiv.org/abs/1904.05801&gt;`_.</span>

<span class="sd">    MDD can measure the distribution discrepancy in domain adaptation.</span>

<span class="sd">    The :math:`y^s` and :math:`y^t` are logits output by the main head on the source and target domain respectively.</span>
<span class="sd">    The :math:`y_{adv}^s` and :math:`y_{adv}^t` are logits output by the adversarial head.</span>

<span class="sd">    The definition can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathcal{D}_{\gamma}(\hat{\mathcal{S}}, \hat{\mathcal{T}}) =</span>
<span class="sd">        -\gamma \mathbb{E}_{y^s, y_{adv}^s \sim\hat{\mathcal{S}}} L_s (y^s, y_{adv}^s) +</span>
<span class="sd">        \mathbb{E}_{y^t, y_{adv}^t \sim\hat{\mathcal{T}}} L_t (y^t, y_{adv}^t),</span>

<span class="sd">    where :math:`\gamma` is a margin hyper-parameter, :math:`L_s` refers to the disparity function defined on the source domain</span>
<span class="sd">    and :math:`L_t` refers to the disparity function defined on the target domain.</span>

<span class="sd">    Args:</span>
<span class="sd">        source_disparity (callable): The disparity function defined on the source domain, :math:`L_s`.</span>
<span class="sd">        target_disparity (callable): The disparity function defined on the target domain, :math:`L_t`.</span>
<span class="sd">        margin (float): margin :math:`\gamma`. Default: 4</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">          ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">          ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">          elements in the output, ``&#39;sum&#39;``: the output will be summed. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - y_s: output :math:`y^s` by the main head on the source domain</span>
<span class="sd">        - y_s_adv: output :math:`y^s` by the adversarial head on the source domain</span>
<span class="sd">        - y_t: output :math:`y^t` by the main head on the target domain</span>
<span class="sd">        - y_t_adv: output :math:`y_{adv}^t` by the adversarial head on the target domain</span>
<span class="sd">        - w_s (optional): instance weights for source domain</span>
<span class="sd">        - w_t (optional): instance weights for target domain</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; num_outputs = 2</span>
<span class="sd">        &gt;&gt;&gt; batch_size = 10</span>
<span class="sd">        &gt;&gt;&gt; loss = MarginDisparityDiscrepancy(margin=4., source_disparity=F.l1_loss, target_disparity=F.l1_loss)</span>
<span class="sd">        &gt;&gt;&gt; # output from source domain and target domain</span>
<span class="sd">        &gt;&gt;&gt; y_s, y_t = torch.randn(batch_size, num_outputs), torch.randn(batch_size, num_outputs)</span>
<span class="sd">        &gt;&gt;&gt; # adversarial output from source domain and target domain</span>
<span class="sd">        &gt;&gt;&gt; y_s_adv, y_t_adv = torch.randn(batch_size, num_outputs), torch.randn(batch_size, num_outputs)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(y_s, y_s_adv, y_t, y_t_adv)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source_disparity</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">target_disparity</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
                 <span class="n">margin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MarginDisparityDiscrepancy</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">source_disparity</span> <span class="o">=</span> <span class="n">source_disparity</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_disparity</span> <span class="o">=</span> <span class="n">target_disparity</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_s</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_s_adv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_t</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_t_adv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
                <span class="n">w_s</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">w_t</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>

        <span class="n">source_loss</span> <span class="o">=</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">source_disparity</span><span class="p">(</span><span class="n">y_s</span><span class="p">,</span> <span class="n">y_s_adv</span><span class="p">)</span>
        <span class="n">target_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_disparity</span><span class="p">(</span><span class="n">y_t</span><span class="p">,</span> <span class="n">y_t_adv</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">w_s</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">w_s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">source_loss</span><span class="p">)</span>
        <span class="n">source_loss</span> <span class="o">=</span> <span class="n">source_loss</span> <span class="o">*</span> <span class="n">w_s</span>
        <span class="k">if</span> <span class="n">w_t</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">w_t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">target_loss</span><span class="p">)</span>
        <span class="n">target_loss</span> <span class="o">=</span> <span class="n">target_loss</span> <span class="o">*</span> <span class="n">w_t</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">source_loss</span> <span class="o">+</span> <span class="n">target_loss</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div>


<div class="viewcode-block" id="ClassificationMarginDisparityDiscrepancy"><a class="viewcode-back" href="../../../tllib/alignment/hypothesis_adversarial.html#tllib.alignment.mdd.ClassificationMarginDisparityDiscrepancy">[docs]</a><span class="k">class</span> <span class="nc">ClassificationMarginDisparityDiscrepancy</span><span class="p">(</span><span class="n">MarginDisparityDiscrepancy</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The margin disparity discrepancy (MDD) proposed in `Bridging Theory and Algorithm for Domain Adaptation (ICML 2019) &lt;https://arxiv.org/abs/1904.05801&gt;`_.</span>

<span class="sd">    It measures the distribution discrepancy in domain adaptation</span>
<span class="sd">    for classification.</span>

<span class="sd">    When margin is equal to 1, it&#39;s also called disparity discrepancy (DD).</span>

<span class="sd">    The :math:`y^s` and :math:`y^t` are logits output by the main classifier on the source and target domain respectively.</span>
<span class="sd">    The :math:`y_{adv}^s` and :math:`y_{adv}^t` are logits output by the adversarial classifier.</span>
<span class="sd">    They are expected to contain raw, unnormalized scores for each class.</span>

<span class="sd">    The definition can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathcal{D}_{\gamma}(\hat{\mathcal{S}}, \hat{\mathcal{T}}) =</span>
<span class="sd">        \gamma \mathbb{E}_{y^s, y_{adv}^s \sim\hat{\mathcal{S}}} \log\left(\frac{\exp(y_{adv}^s[h_{y^s}])}{\sum_j \exp(y_{adv}^s[j])}\right) +</span>
<span class="sd">        \mathbb{E}_{y^t, y_{adv}^t \sim\hat{\mathcal{T}}} \log\left(1-\frac{\exp(y_{adv}^t[h_{y^t}])}{\sum_j \exp(y_{adv}^t[j])}\right),</span>

<span class="sd">    where :math:`\gamma` is a margin hyper-parameter and :math:`h_y` refers to the predicted label when the logits output is :math:`y`.</span>
<span class="sd">    You can see more details in `Bridging Theory and Algorithm for Domain Adaptation &lt;https://arxiv.org/abs/1904.05801&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float): margin :math:`\gamma`. Default: 4</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">          ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">          ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">          elements in the output, ``&#39;sum&#39;``: the output will be summed. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - y_s: logits output :math:`y^s` by the main classifier on the source domain</span>
<span class="sd">        - y_s_adv: logits output :math:`y^s` by the adversarial classifier on the source domain</span>
<span class="sd">        - y_t: logits output :math:`y^t` by the main classifier on the target domain</span>
<span class="sd">        - y_t_adv: logits output :math:`y_{adv}^t` by the adversarial classifier on the target domain</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Inputs: :math:`(minibatch, C)` where C = number of classes, or :math:`(minibatch, C, d_1, d_2, ..., d_K)`</span>
<span class="sd">          with :math:`K \geq 1` in the case of `K`-dimensional loss.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then the same size as the target: :math:`(minibatch)`, or</span>
<span class="sd">          :math:`(minibatch, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of K-dimensional loss.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; num_classes = 2</span>
<span class="sd">        &gt;&gt;&gt; batch_size = 10</span>
<span class="sd">        &gt;&gt;&gt; loss = ClassificationMarginDisparityDiscrepancy(margin=4.)</span>
<span class="sd">        &gt;&gt;&gt; # logits output from source domain and target domain</span>
<span class="sd">        &gt;&gt;&gt; y_s, y_t = torch.randn(batch_size, num_classes), torch.randn(batch_size, num_classes)</span>
<span class="sd">        &gt;&gt;&gt; # adversarial logits output from source domain and target domain</span>
<span class="sd">        &gt;&gt;&gt; y_s_adv, y_t_adv = torch.randn(batch_size, num_classes), torch.randn(batch_size, num_classes)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(y_s, y_s_adv, y_t, y_t_adv)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">source_discrepancy</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_adv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y_adv</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">target_discrepancy</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_adv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">prediction</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">shift_log</span><span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y_adv</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)),</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">ClassificationMarginDisparityDiscrepancy</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">source_discrepancy</span><span class="p">,</span> <span class="n">target_discrepancy</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span>
                                                                       <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="RegressionMarginDisparityDiscrepancy"><a class="viewcode-back" href="../../../tllib/alignment/hypothesis_adversarial.html#tllib.alignment.mdd.RegressionMarginDisparityDiscrepancy">[docs]</a><span class="k">class</span> <span class="nc">RegressionMarginDisparityDiscrepancy</span><span class="p">(</span><span class="n">MarginDisparityDiscrepancy</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The margin disparity discrepancy (MDD) proposed in `Bridging Theory and Algorithm for Domain Adaptation (ICML 2019) &lt;https://arxiv.org/abs/1904.05801&gt;`_.</span>

<span class="sd">    It measures the distribution discrepancy in domain adaptation</span>
<span class="sd">    for regression.</span>

<span class="sd">    The :math:`y^s` and :math:`y^t` are logits output by the main regressor on the source and target domain respectively.</span>
<span class="sd">    The :math:`y_{adv}^s` and :math:`y_{adv}^t` are logits output by the adversarial regressor.</span>
<span class="sd">    They are expected to contain ``normalized`` values for each factors.</span>

<span class="sd">    The definition can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \mathcal{D}_{\gamma}(\hat{\mathcal{S}}, \hat{\mathcal{T}}) =</span>
<span class="sd">        -\gamma \mathbb{E}_{y^s, y_{adv}^s \sim\hat{\mathcal{S}}} L (y^s, y_{adv}^s) +</span>
<span class="sd">        \mathbb{E}_{y^t, y_{adv}^t \sim\hat{\mathcal{T}}} L (y^t, y_{adv}^t),</span>

<span class="sd">    where :math:`\gamma` is a margin hyper-parameter and :math:`L` refers to the disparity function defined on both domains.</span>
<span class="sd">    You can see more details in `Bridging Theory and Algorithm for Domain Adaptation &lt;https://arxiv.org/abs/1904.05801&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        loss_function (callable): The disparity function defined on both domains, :math:`L`.</span>
<span class="sd">        margin (float): margin :math:`\gamma`. Default: 1</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">          ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">          ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">          elements in the output, ``&#39;sum&#39;``: the output will be summed. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - y_s: logits output :math:`y^s` by the main regressor on the source domain</span>
<span class="sd">        - y_s_adv: logits output :math:`y^s` by the adversarial regressor on the source domain</span>
<span class="sd">        - y_t: logits output :math:`y^t` by the main regressor on the target domain</span>
<span class="sd">        - y_t_adv: logits output :math:`y_{adv}^t` by the adversarial regressor on the target domain</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Inputs: :math:`(minibatch, F)` where F = number of factors, or :math:`(minibatch, F, d_1, d_2, ..., d_K)`</span>
<span class="sd">          with :math:`K \geq 1` in the case of `K`-dimensional loss.</span>
<span class="sd">        - Output: scalar. The same size as the target: :math:`(minibatch)`, or</span>
<span class="sd">          :math:`(minibatch, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of K-dimensional loss.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; num_outputs = 2</span>
<span class="sd">        &gt;&gt;&gt; batch_size = 10</span>
<span class="sd">        &gt;&gt;&gt; loss = RegressionMarginDisparityDiscrepancy(margin=4., loss_function=F.l1_loss)</span>
<span class="sd">        &gt;&gt;&gt; # output from source domain and target domain</span>
<span class="sd">        &gt;&gt;&gt; y_s, y_t = torch.randn(batch_size, num_outputs), torch.randn(batch_size, num_outputs)</span>
<span class="sd">        &gt;&gt;&gt; # adversarial output from source domain and target domain</span>
<span class="sd">        &gt;&gt;&gt; y_s_adv, y_t_adv = torch.randn(batch_size, num_outputs), torch.randn(batch_size, num_outputs)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(y_s, y_s_adv, y_t, y_t_adv)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss_function</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">source_discrepancy</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_adv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">y_adv</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">target_discrepancy</span><span class="p">(</span><span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y_adv</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">y_adv</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">RegressionMarginDisparityDiscrepancy</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">source_discrepancy</span><span class="p">,</span> <span class="n">target_discrepancy</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span>
                                                                   <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="shift_log"><a class="viewcode-back" href="../../../tllib/alignment/hypothesis_adversarial.html#tllib.alignment.mdd.shift_log">[docs]</a><span class="k">def</span> <span class="nf">shift_log</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">offset</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    First shift, then calculate log, which can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        y = \max(\log(x+\text{offset}), 0)</span>

<span class="sd">    Used to avoid the gradient explosion problem in log(x) function when x=0.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): input tensor</span>
<span class="sd">        offset (float, optional): offset size. Default: 1e-6</span>

<span class="sd">    .. note::</span>
<span class="sd">        Input tensor falls in [0., 1.] and the output tensor falls in [-log(offset), 0]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">offset</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1.</span><span class="p">))</span></div>


<span class="k">class</span> <span class="nc">GeneralModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bottleneck</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
                 <span class="n">head</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">adv_head</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">grl</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">WarmStartGradientReverseLayer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">finetune</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GeneralModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">backbone</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span> <span class="o">=</span> <span class="n">bottleneck</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adv_head</span> <span class="o">=</span> <span class="n">adv_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">finetune</span> <span class="o">=</span> <span class="n">finetune</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grl_layer</span> <span class="o">=</span> <span class="n">WarmStartGradientReverseLayer</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lo</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">hi</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                                       <span class="n">auto_step</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">if</span> <span class="n">grl</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">grl</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;&quot;&quot;&quot;</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">features_adv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">grl_layer</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">outputs_adv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adv_head</span><span class="p">(</span><span class="n">features_adv</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">outputs_adv</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Gradually increase :math:`\lambda` in GRL layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">grl_layer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a parameters list which decides optimization hyper-parameters,</span>
<span class="sd">        such as the relative learning rate of each layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">base_lr</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">finetune</span> <span class="k">else</span> <span class="n">base_lr</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bottleneck</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">base_lr</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">base_lr</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;params&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">adv_head</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">base_lr</span><span class="p">}</span>
        <span class="p">]</span>
        <span class="k">return</span> <span class="n">params</span>


<div class="viewcode-block" id="ImageClassifier"><a class="viewcode-back" href="../../../tllib/alignment/hypothesis_adversarial.html#tllib.alignment.mdd.ImageClassifier">[docs]</a><span class="k">class</span> <span class="nc">ImageClassifier</span><span class="p">(</span><span class="n">GeneralModule</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Classifier for MDD.</span>

<span class="sd">    Classifier for MDD has one backbone, one bottleneck, while two classifier heads.</span>
<span class="sd">    The first classifier head is used for final predictions.</span>
<span class="sd">    The adversarial classifier head is only used when calculating MarginDisparityDiscrepancy.</span>


<span class="sd">    Args:</span>
<span class="sd">        backbone (torch.nn.Module): Any backbone to extract 1-d features from data</span>
<span class="sd">        num_classes (int): Number of classes</span>
<span class="sd">        bottleneck_dim (int, optional): Feature dimension of the bottleneck layer. Default: 1024</span>
<span class="sd">        width (int, optional): Feature dimension of the classifier head. Default: 1024</span>
<span class="sd">        grl (nn.Module): Gradient reverse layer. Will use default parameters if None. Default: None.</span>
<span class="sd">        finetune (bool, optional): Whether use 10x smaller learning rate in the backbone. Default: True</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - x (tensor): input data</span>

<span class="sd">    Outputs:</span>
<span class="sd">        - outputs: logits outputs by the main classifier</span>
<span class="sd">        - outputs_adv: logits outputs by the adversarial classifier</span>

<span class="sd">    Shape:</span>
<span class="sd">        - x: :math:`(minibatch, *)`, same shape as the input of the `backbone`.</span>
<span class="sd">        - outputs, outputs_adv: :math:`(minibatch, C)`, where C means the number of classes.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Remember to call function `step()` after function `forward()` **during training phase**! For instance,</span>

<span class="sd">            &gt;&gt;&gt; # x is inputs, classifier is an ImageClassifier</span>
<span class="sd">            &gt;&gt;&gt; outputs, outputs_adv = classifier(x)</span>
<span class="sd">            &gt;&gt;&gt; classifier.step()</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                 <span class="n">bottleneck_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">width</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span>
                 <span class="n">grl</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">WarmStartGradientReverseLayer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">finetune</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pool_layer</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">grl_layer</span> <span class="o">=</span> <span class="n">WarmStartGradientReverseLayer</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lo</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">hi</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                                       <span class="n">auto_step</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">if</span> <span class="n">grl</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">grl</span>

        <span class="k">if</span> <span class="n">pool_layer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">pool_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="n">bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">pool_layer</span><span class="p">,</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">backbone</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">bottleneck_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">bottleneck_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">bottleneck</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">)</span>
        <span class="n">bottleneck</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

        <span class="c1"># The classifier head used for final predictions.</span>
        <span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">width</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># The adversarial classifier head</span>
        <span class="n">adv_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">width</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">dep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">head</span><span class="p">[</span><span class="n">dep</span> <span class="o">*</span> <span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
            <span class="n">head</span><span class="p">[</span><span class="n">dep</span> <span class="o">*</span> <span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
            <span class="n">adv_head</span><span class="p">[</span><span class="n">dep</span> <span class="o">*</span> <span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
            <span class="n">adv_head</span><span class="p">[</span><span class="n">dep</span> <span class="o">*</span> <span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ImageClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">bottleneck</span><span class="p">,</span>
                                              <span class="n">head</span><span class="p">,</span> <span class="n">adv_head</span><span class="p">,</span> <span class="n">grl_layer</span><span class="p">,</span> <span class="n">finetune</span><span class="p">)</span></div>


<div class="viewcode-block" id="ImageRegressor"><a class="viewcode-back" href="../../../tllib/alignment/hypothesis_adversarial.html#tllib.alignment.mdd.ImageRegressor">[docs]</a><span class="k">class</span> <span class="nc">ImageRegressor</span><span class="p">(</span><span class="n">GeneralModule</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Regressor for MDD.</span>

<span class="sd">    Regressor for MDD has one backbone, one bottleneck, while two regressor heads.</span>
<span class="sd">    The first regressor head is used for final predictions.</span>
<span class="sd">    The adversarial regressor head is only used when calculating MarginDisparityDiscrepancy.</span>


<span class="sd">    Args:</span>
<span class="sd">        backbone (torch.nn.Module): Any backbone to extract 1-d features from data</span>
<span class="sd">        num_factors (int): Number of factors</span>
<span class="sd">        bottleneck_dim (int, optional): Feature dimension of the bottleneck layer. Default: 1024</span>
<span class="sd">        width (int, optional): Feature dimension of the classifier head. Default: 1024</span>
<span class="sd">        finetune (bool, optional): Whether use 10x smaller learning rate in the backbone. Default: True</span>

<span class="sd">    Inputs:</span>
<span class="sd">        - x (Tensor): input data</span>

<span class="sd">    Outputs: (outputs, outputs_adv)</span>
<span class="sd">        - outputs: outputs by the main regressor</span>
<span class="sd">        - outputs_adv: outputs by the adversarial regressor</span>

<span class="sd">    Shape:</span>
<span class="sd">        - x: :math:`(minibatch, *)`, same shape as the input of the `backbone`.</span>
<span class="sd">        - outputs, outputs_adv: :math:`(minibatch, F)`, where F means the number of factors.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Remember to call function `step()` after function `forward()` **during training phase**! For instance,</span>

<span class="sd">            &gt;&gt;&gt; # x is inputs, regressor is an ImageRegressor</span>
<span class="sd">            &gt;&gt;&gt; outputs, outputs_adv = regressor(x)</span>
<span class="sd">            &gt;&gt;&gt; regressor.step()</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">backbone</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">num_factors</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bottleneck</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">adv_head</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bottleneck_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">width</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">finetune</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">grl_layer</span> <span class="o">=</span> <span class="n">WarmStartGradientReverseLayer</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">lo</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">hi</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">auto_step</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bottleneck</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">bottleneck</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">backbone</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">bottleneck_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="p">)</span>

        <span class="c1"># The regressor head used for final predictions.</span>
        <span class="k">if</span> <span class="n">head</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">width</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">width</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">num_factors</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">head</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># The adversarial regressor head</span>
        <span class="k">if</span> <span class="n">adv_head</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">adv_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">bottleneck_dim</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">width</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">width</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="n">output_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">num_factors</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">adv_head</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
                    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ImageRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">backbone</span><span class="p">,</span> <span class="n">num_factors</span><span class="p">,</span> <span class="n">bottleneck</span><span class="p">,</span>
                                              <span class="n">head</span><span class="p">,</span> <span class="n">adv_head</span><span class="p">,</span> <span class="n">grl_layer</span><span class="p">,</span> <span class="n">finetune</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_factors</span> <span class="o">=</span> <span class="n">num_factors</span></div>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright THUML Group.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://github.com/pytorch/pytorch_sphinx_theme">PyTorch Sphinx Theme</a>.
      </div>
      <!-- <div>
        <a href="https://beian.miit.gov.cn/">ICP16023543-1</a>
      </div> -->
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/language_data.js"></script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!-- <div class="col-md-4 text-center">
          <h2>Survey</h2>
          <p>Access comprehensive survey for transfer learning</p>
          <a class="with-right-arrow" href="">View Survey</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Papers</h2>
          <p>Access comprehensive paper list for transfer learning</p>
          <a class="with-right-arrow" href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">View Paper List</a>
        </div> -->

        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive documentation for Transfer Learning Library</p>
          <a class="with-right-arrow" href="transfer.thuml.ai">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get started for Transfer Learning Library</p>
          <a class="with-right-arrow" href="http://microhhh.com/get_started/installing.html">Get Started</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Paper List</h2>
          <p>Get started for transfer learning</p>
          <a class="with-right-arrow" href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">View Resources</a>
        </div>
      </div>
    </div>
  </div>



  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebooks Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="transfer.thuml.ai" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="http://microhhh.com/get_started/installing.html">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="transfer.thuml.ai">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/thuml/Transfer-Learning-Library/">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>