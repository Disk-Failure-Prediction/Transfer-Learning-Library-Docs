


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Datasets &mdash; Transfer Learning Library 0.0.24 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Models" href="models.html" />
    <link rel="prev" title="Vision" href="index.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
<!--      pytorch logo -->
<!--      <a class="header-logo" href="transfer.thuml.ai" aria-label="PyTorch"></a>-->

      <div class="main-menu">
        <ul>
          <!-- <li>
            <a href="">Full Survey</a>
          </li>
          <li>
            <a href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">Paper List</a>
          </li> -->
          <li>
            <li>
              <a href="transfer.thuml.ai">Docs</a>
            </li>
            
            <li>
              <a href="https://github.com/thuml/Transfer-Learning-Library/">Github</a>
            </li>

            <li>
              <a href="https://arxiv.org/abs/2201.05867">Survey</a>
            </li>

          <li>
            <a href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">Paper List</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Transfer Learning API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../alignment/index.html">Feature Alignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../translation.html">Domain Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../self_training.html">Self Training Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reweight.html">Re-weighting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../normalization.html">Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../regularization.html">Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ranking.html">Ranking</a></li>
</ul>
<p class="caption"><span class="caption-text">Common API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/index.html">Utilities</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">Vision</a> &gt;</li>
        
      <li>Datasets</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/tllib/vision/datasets.rst.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="datasets">
<h1>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h1>
<div class="section" id="cross-domain-classification">
<h2>Cross-Domain Classification<a class="headerlink" href="#cross-domain-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="imagelist">
<h3>ImageList<a class="headerlink" href="#imagelist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.imagelist.ImageList">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.imagelist.</code><code class="sig-name descname">ImageList</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">classes</em>, <em class="sig-param">data_list_file</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/imagelist.html#ImageList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.imagelist.ImageList" title="Permalink to this definition">¶</a></dt>
<dd><p>A generic Dataset class for image classification</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>classes</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>]</em>) – The names of all the classes</p></li>
<li><p><strong>data_list_file</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – File to read the image list from.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image             and returns a transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>data_list_file</cite>, each line has 2 values in the following format.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source_dir</span><span class="o">/</span><span class="n">dog_xxx</span><span class="o">.</span><span class="n">png</span> <span class="mi">0</span>
<span class="n">source_dir</span><span class="o">/</span><span class="n">cat_123</span><span class="o">.</span><span class="n">png</span> <span class="mi">1</span>
<span class="n">target_dir</span><span class="o">/</span><span class="n">dog_xxy</span><span class="o">.</span><span class="n">png</span> <span class="mi">0</span>
<span class="n">target_dir</span><span class="o">/</span><span class="n">cat_nsdf3</span><span class="o">.</span><span class="n">png</span> <span class="mi">1</span>
</pre></div>
</div>
<p>The first value is the relative path of an image, and the second value is the label of the corresponding image.
If your data_list_file has different formats, please over-ride <a class="reference internal" href="#tllib.vision.datasets.imagelist.ImageList.parse_data_file" title="tllib.vision.datasets.imagelist.ImageList.parse_data_file"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse_data_file()</span></code></a>.</p>
</div>
<dl class="method">
<dt id="tllib.vision.datasets.imagelist.ImageList.domains">
<em class="property">classmethod </em><code class="sig-name descname">domains</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/imagelist.html#ImageList.domains"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.imagelist.ImageList.domains" title="Permalink to this definition">¶</a></dt>
<dd><p>All possible domain in this dataset</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.imagelist.ImageList.num_classes">
<em class="property">property </em><code class="sig-name descname">num_classes</code><a class="headerlink" href="#tllib.vision.datasets.imagelist.ImageList.num_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of classes</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.imagelist.ImageList.parse_data_file">
<code class="sig-name descname">parse_data_file</code><span class="sig-paren">(</span><em class="sig-param">file_name</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/imagelist.html#ImageList.parse_data_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.imagelist.ImageList.parse_data_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse file to data list</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The path of data file</p></li>
<li><p><strong>return</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – List of (image path, class_index) tuples</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="office-31">
<h3>Office-31<a class="headerlink" href="#office-31" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.office31.Office31">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.office31.</code><code class="sig-name descname">Office31</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/office31.html#Office31"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.office31.Office31" title="Permalink to this definition">¶</a></dt>
<dd><p>Office31 Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'A'</span></code>: amazon,             <code class="docutils literal notranslate"><span class="pre">'D'</span></code>: dslr and <code class="docutils literal notranslate"><span class="pre">'W'</span></code>: webcam.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">amazon</span><span class="o">/</span>
    <span class="n">images</span><span class="o">/</span>
        <span class="n">backpack</span><span class="o">/</span>
            <span class="o">*.</span><span class="n">jpg</span>
            <span class="o">...</span>
<span class="n">dslr</span><span class="o">/</span>
<span class="n">webcam</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">amazon</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">dslr</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">webcam</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<dl class="method">
<dt id="tllib.vision.datasets.office31.Office31.domains">
<em class="property">classmethod </em><code class="sig-name descname">domains</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/office31.html#Office31.domains"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.office31.Office31.domains" title="Permalink to this definition">¶</a></dt>
<dd><p>All possible domain in this dataset</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.office31.Office31.num_classes">
<em class="property">property </em><code class="sig-name descname">num_classes</code><a class="headerlink" href="#tllib.vision.datasets.office31.Office31.num_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of classes</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.office31.Office31.parse_data_file">
<code class="sig-name descname">parse_data_file</code><span class="sig-paren">(</span><em class="sig-param">file_name</em><span class="sig-paren">)</span><a class="headerlink" href="#tllib.vision.datasets.office31.Office31.parse_data_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse file to data list</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The path of data file</p></li>
<li><p><strong>return</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – List of (image path, class_index) tuples</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="office-caltech">
<h3>Office-Caltech<a class="headerlink" href="#office-caltech" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.officecaltech.OfficeCaltech">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.officecaltech.</code><code class="sig-name descname">OfficeCaltech</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/officecaltech.html#OfficeCaltech"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.officecaltech.OfficeCaltech" title="Permalink to this definition">¶</a></dt>
<dd><p>Office+Caltech Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'A'</span></code>: amazon,             <code class="docutils literal notranslate"><span class="pre">'D'</span></code>: dslr, <code class="docutils literal notranslate"><span class="pre">'W'</span></code>:webcam and <code class="docutils literal notranslate"><span class="pre">'C'</span></code>: caltech.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">amazon</span><span class="o">/</span>
    <span class="n">images</span><span class="o">/</span>
        <span class="n">backpack</span><span class="o">/</span>
            <span class="o">*.</span><span class="n">jpg</span>
            <span class="o">...</span>
<span class="n">dslr</span><span class="o">/</span>
<span class="n">webcam</span><span class="o">/</span>
<span class="n">caltech</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">amazon</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">dslr</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">webcam</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">caltech</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<dl class="method">
<dt id="tllib.vision.datasets.officecaltech.OfficeCaltech.num_classes">
<em class="property">property </em><code class="sig-name descname">num_classes</code><a class="headerlink" href="#tllib.vision.datasets.officecaltech.OfficeCaltech.num_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of classes</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="office-home">
<h3>Office-Home<a class="headerlink" href="#office-home" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.officehome.OfficeHome">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.officehome.</code><code class="sig-name descname">OfficeHome</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/officehome.html#OfficeHome"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.officehome.OfficeHome" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://hemanthdv.org/OfficeHome-Dataset/">OfficeHome</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'Ar'</span></code>: Art,             <code class="docutils literal notranslate"><span class="pre">'Cl'</span></code>: Clipart, <code class="docutils literal notranslate"><span class="pre">'Pr'</span></code>: Product and <code class="docutils literal notranslate"><span class="pre">'Rw'</span></code>: Real_World.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Art</span><span class="o">/</span>
    <span class="n">Alarm_Clock</span><span class="o">/*.</span><span class="n">jpg</span>
    <span class="o">...</span>
<span class="n">Clipart</span><span class="o">/</span>
<span class="n">Product</span><span class="o">/</span>
<span class="n">Real_World</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">Art</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">Clipart</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">Product</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">Real_World</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<dl class="method">
<dt id="tllib.vision.datasets.officehome.OfficeHome.domains">
<em class="property">classmethod </em><code class="sig-name descname">domains</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/officehome.html#OfficeHome.domains"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.officehome.OfficeHome.domains" title="Permalink to this definition">¶</a></dt>
<dd><p>All possible domain in this dataset</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.officehome.OfficeHome.num_classes">
<em class="property">property </em><code class="sig-name descname">num_classes</code><a class="headerlink" href="#tllib.vision.datasets.officehome.OfficeHome.num_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of classes</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.officehome.OfficeHome.parse_data_file">
<code class="sig-name descname">parse_data_file</code><span class="sig-paren">(</span><em class="sig-param">file_name</em><span class="sig-paren">)</span><a class="headerlink" href="#tllib.vision.datasets.officehome.OfficeHome.parse_data_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse file to data list</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The path of data file</p></li>
<li><p><strong>return</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – List of (image path, class_index) tuples</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="visda-2017">
<h3>VisDA-2017<a class="headerlink" href="#visda-2017" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.visda2017.VisDA2017">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.visda2017.</code><code class="sig-name descname">VisDA2017</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/visda2017.html#VisDA2017"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.visda2017.VisDA2017" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://ai.bu.edu/visda-2017/assets/attachments/VisDA_2017.pdf">VisDA-2017</a> Dataset</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'Synthetic'</span></code>: synthetic images and             <code class="docutils literal notranslate"><span class="pre">'Real'</span></code>: real-world images.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
    <span class="n">aeroplance</span><span class="o">/</span>
        <span class="o">*.</span><span class="n">png</span>
        <span class="o">...</span>
<span class="n">validation</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">train</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">validation</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<dl class="method">
<dt id="tllib.vision.datasets.visda2017.VisDA2017.domains">
<em class="property">classmethod </em><code class="sig-name descname">domains</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/visda2017.html#VisDA2017.domains"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.visda2017.VisDA2017.domains" title="Permalink to this definition">¶</a></dt>
<dd><p>All possible domain in this dataset</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.visda2017.VisDA2017.num_classes">
<em class="property">property </em><code class="sig-name descname">num_classes</code><a class="headerlink" href="#tllib.vision.datasets.visda2017.VisDA2017.num_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of classes</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.visda2017.VisDA2017.parse_data_file">
<code class="sig-name descname">parse_data_file</code><span class="sig-paren">(</span><em class="sig-param">file_name</em><span class="sig-paren">)</span><a class="headerlink" href="#tllib.vision.datasets.visda2017.VisDA2017.parse_data_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse file to data list</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The path of data file</p></li>
<li><p><strong>return</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – List of (image path, class_index) tuples</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="domainnet">
<h3>DomainNet<a class="headerlink" href="#domainnet" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.domainnet.DomainNet">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.domainnet.</code><code class="sig-name descname">DomainNet</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/domainnet.html#DomainNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.domainnet.DomainNet" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://ai.bu.edu/M3SDA/#dataset">DomainNet</a> (cleaned version, recommended)</p>
<p>See <a class="reference external" href="https://arxiv.org/abs/1812.01754">Moment Matching for Multi-Source Domain Adaptation</a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'c'</span></code>:clipart,             <code class="docutils literal notranslate"><span class="pre">'i'</span></code>: infograph, <code class="docutils literal notranslate"><span class="pre">'p'</span></code>: painting, <code class="docutils literal notranslate"><span class="pre">'q'</span></code>: quickdraw, <code class="docutils literal notranslate"><span class="pre">'r'</span></code>: real, <code class="docutils literal notranslate"><span class="pre">'s'</span></code>: sketch</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clipart</span><span class="o">/</span>
<span class="n">infograph</span><span class="o">/</span>
<span class="n">painting</span><span class="o">/</span>
<span class="n">quickdraw</span><span class="o">/</span>
<span class="n">real</span><span class="o">/</span>
<span class="n">sketch</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">clipart</span><span class="o">.</span><span class="n">txt</span>
    <span class="o">...</span>
</pre></div>
</div>
</div>
<dl class="method">
<dt id="tllib.vision.datasets.domainnet.DomainNet.domains">
<em class="property">classmethod </em><code class="sig-name descname">domains</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/domainnet.html#DomainNet.domains"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.domainnet.DomainNet.domains" title="Permalink to this definition">¶</a></dt>
<dd><p>All possible domain in this dataset</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.domainnet.DomainNet.num_classes">
<em class="property">property </em><code class="sig-name descname">num_classes</code><a class="headerlink" href="#tllib.vision.datasets.domainnet.DomainNet.num_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of classes</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.domainnet.DomainNet.parse_data_file">
<code class="sig-name descname">parse_data_file</code><span class="sig-paren">(</span><em class="sig-param">file_name</em><span class="sig-paren">)</span><a class="headerlink" href="#tllib.vision.datasets.domainnet.DomainNet.parse_data_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse file to data list</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The path of data file</p></li>
<li><p><strong>return</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – List of (image path, class_index) tuples</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="pacs">
<h3>PACS<a class="headerlink" href="#pacs" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.pacs.PACS">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.pacs.</code><code class="sig-name descname">PACS</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">split='all'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/pacs.html#PACS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.pacs.PACS" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://domaingeneralization.github.io/#data">PACS Dataset</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'A'</span></code>: amazon,             <code class="docutils literal notranslate"><span class="pre">'D'</span></code>: dslr and <code class="docutils literal notranslate"><span class="pre">'W'</span></code>: webcam.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">art_painting</span><span class="o">/</span>
    <span class="n">dog</span><span class="o">/</span>
        <span class="o">*.</span><span class="n">jpg</span>
        <span class="o">...</span>
<span class="n">cartoon</span><span class="o">/</span>
<span class="n">photo</span><span class="o">/</span>
<span class="n">sketch</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">art_painting</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">cartoon</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">photo</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">sketch</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
<dl class="method">
<dt id="tllib.vision.datasets.pacs.PACS.domains">
<em class="property">classmethod </em><code class="sig-name descname">domains</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/pacs.html#PACS.domains"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.pacs.PACS.domains" title="Permalink to this definition">¶</a></dt>
<dd><p>All possible domain in this dataset</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="mnist">
<h3>MNIST<a class="headerlink" href="#mnist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.digits.MNIST">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.digits.</code><code class="sig-name descname">MNIST</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">mode='L'</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/digits.html#MNIST"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.digits.MNIST" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset where <code class="docutils literal notranslate"><span class="pre">MNIST/processed/training.pt</span></code>
and  <code class="docutils literal notranslate"><span class="pre">MNIST/processed/test.pt</span></code> exist.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The channel mode for image. Choices includes <code class="docutils literal notranslate"><span class="pre">&quot;L&quot;`</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;RGB&quot;</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;L&quot;`</span></code></p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="usps">
<h3>USPS<a class="headerlink" href="#usps" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.digits.USPS">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.digits.</code><code class="sig-name descname">USPS</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">mode='L'</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/digits.html#USPS"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.digits.USPS" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt><a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps">USPS</a> Dataset.</dt><dd><p>The data-format is : [label [index:value ]*256 n] * num_lines, where <code class="docutils literal notranslate"><span class="pre">label</span></code> lies in <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">10]</span></code>.
The value for each pixel lies in <code class="docutils literal notranslate"><span class="pre">[-1,</span> <span class="pre">1]</span></code>. Here we transform the <code class="docutils literal notranslate"><span class="pre">label</span></code> into <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">9]</span></code>
and make pixel values in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code>.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset to store``USPS`` data files.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The channel mode for image. Choices includes <code class="docutils literal notranslate"><span class="pre">&quot;L&quot;`</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;RGB&quot;</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;L&quot;`</span></code></p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="svhn">
<h3>SVHN<a class="headerlink" href="#svhn" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.digits.SVHN">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.digits.</code><code class="sig-name descname">SVHN</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">mode='L'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/digits.html#SVHN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.digits.SVHN" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://ufldl.stanford.edu/housenumbers/">SVHN</a> Dataset.
Note: The SVHN dataset assigns the label <cite>10</cite> to the digit <cite>0</cite>. However, in this Dataset,
we assign the label <cite>0</cite> to the digit <cite>0</cite> to be compatible with PyTorch loss functions which
expect the class labels to be in the range <cite>[0, C-1]</cite></p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This class needs <a class="reference external" href="https://docs.scipy.org/doc/">scipy</a> to load data from <cite>.mat</cite> format.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset where directory
<code class="docutils literal notranslate"><span class="pre">SVHN</span></code> exists.</p></li>
<li><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The channel mode for image. Choices includes <code class="docutils literal notranslate"><span class="pre">&quot;L&quot;`</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;RGB&quot;</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">&quot;RGB&quot;`</span></code></p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image
and returns a transformed version. E.g, <code class="docutils literal notranslate"><span class="pre">transforms.RandomCrop</span></code></p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and
puts it in root directory. If dataset is already downloaded, it is not
downloaded again.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="partial-cross-domain-classification">
<h2>Partial Cross-Domain Classification<a class="headerlink" href="#partial-cross-domain-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="partial-wrapper">
<h3>Partial Wrapper<a class="headerlink" href="#partial-wrapper" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="tllib.vision.datasets.partial.partial">
<code class="sig-prename descclassname">tllib.vision.datasets.partial.</code><code class="sig-name descname">partial</code><span class="sig-paren">(</span><em class="sig-param">dataset_class</em>, <em class="sig-param">partial_classes</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/partial.html#partial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.partial.partial" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a dataset into its partial version.</p>
<p>In other words, those samples which doesn’t belong to <cite>partial_classes</cite> will be discarded.
Yet <cite>partial</cite> will not change the label space of <cite>dataset_class</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset_class</strong> (<em>class</em>) – Dataset class. Only subclass of <code class="docutils literal notranslate"><span class="pre">ImageList</span></code> can be partial.</p></li>
<li><p><strong>partial_classes</strong> (<em>sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>]</em>) – A sequence of which categories need to be kept in the partial dataset.            Each element of <cite>partial_classes</cite> must belong to the <cite>classes</cite> list of <cite>dataset_class</cite>.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">partial_classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;back_pack&#39;</span><span class="p">,</span> <span class="s1">&#39;bike&#39;</span><span class="p">,</span> <span class="s1">&#39;calculator&#39;</span><span class="p">,</span> <span class="s1">&#39;headphones&#39;</span><span class="p">,</span> <span class="s1">&#39;keyboard&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># create a partial dataset class</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">PartialOffice31</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">Office31</span><span class="p">,</span> <span class="n">partial_classes</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># create an instance of the partial dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">PartialDataset</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;data/office31&quot;</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;A&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="tllib.vision.datasets.partial.default_partial">
<code class="sig-prename descclassname">tllib.vision.datasets.partial.</code><code class="sig-name descname">default_partial</code><span class="sig-paren">(</span><em class="sig-param">dataset_class</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/partial.html#default_partial"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.partial.default_partial" title="Permalink to this definition">¶</a></dt>
<dd><p>Default partial used in some paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataset_class</strong> (<em>class</em>) – Dataset class. Currently, dataset_class must be one of
<a class="reference internal" href="#tllib.vision.datasets.office31.Office31" title="tllib.vision.datasets.office31.Office31"><code class="xref py py-class docutils literal notranslate"><span class="pre">Office31</span></code></a>, <a class="reference internal" href="#tllib.vision.datasets.officehome.OfficeHome" title="tllib.vision.datasets.officehome.OfficeHome"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfficeHome</span></code></a>,
<a class="reference internal" href="#tllib.vision.datasets.visda2017.VisDA2017" title="tllib.vision.datasets.visda2017.VisDA2017"><code class="xref py py-class docutils literal notranslate"><span class="pre">VisDA2017</span></code></a>,
<a class="reference internal" href="#tllib.vision.datasets.partial.imagenet_caltech.ImageNetCaltech" title="tllib.vision.datasets.partial.imagenet_caltech.ImageNetCaltech"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImageNetCaltech</span></code></a>
and <a class="reference internal" href="#tllib.vision.datasets.partial.caltech_imagenet.CaltechImageNet" title="tllib.vision.datasets.partial.caltech_imagenet.CaltechImageNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">CaltechImageNet</span></code></a>.</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="caltech-256-imagenet-1k">
<h3>Caltech-256-&gt;ImageNet-1k<a class="headerlink" href="#caltech-256-imagenet-1k" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.partial.caltech_imagenet.CaltechImageNet">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.partial.caltech_imagenet.</code><code class="sig-name descname">CaltechImageNet</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/partial/caltech_imagenet.html#CaltechImageNet"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.partial.caltech_imagenet.CaltechImageNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Caltech-ImageNet is constructed from <a class="reference external" href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">Caltech-256</a> and
<a class="reference external" href="http://image-net.org/">ImageNet-1K</a> .</p>
<p>They share 84 common classes. Caltech-ImageNet keeps all classes of Caltech-256.
The label is based on the Caltech256 (class 0-255) . The private classes of ImageNet-1K is discarded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'C'</span></code>:Caltech-256,             <code class="docutils literal notranslate"><span class="pre">'I'</span></code>: ImageNet-1K validation set.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to put <code class="docutils literal notranslate"><span class="pre">train</span></code> and <code class="docutils literal notranslate"><span class="pre">val</span></code> directory of ImageNet-1K manually in <cite>root</cite> directory
since ImageNet-1K is no longer publicly accessible. DALIB will only download Caltech-256 and ImageList automatically.
In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
    <span class="n">n01440764</span><span class="o">/</span>
    <span class="o">...</span>
<span class="n">val</span><span class="o">/</span>
<span class="mi">256</span><span class="n">_ObjectCategories</span><span class="o">/</span>
    <span class="mf">001.</span><span class="n">ak47</span><span class="o">/</span>
    <span class="o">...</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">caltech_256_list</span><span class="o">.</span><span class="n">txt</span>
    <span class="o">...</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="imagenet-1k-caltech-256">
<h3>ImageNet-1k-&gt;Caltech-256<a class="headerlink" href="#imagenet-1k-caltech-256" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.partial.imagenet_caltech.ImageNetCaltech">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.partial.imagenet_caltech.</code><code class="sig-name descname">ImageNetCaltech</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/partial/imagenet_caltech.html#ImageNetCaltech"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.partial.imagenet_caltech.ImageNetCaltech" title="Permalink to this definition">¶</a></dt>
<dd><p>ImageNet-Caltech is constructed from <a class="reference external" href="http://www.vision.caltech.edu/Image_Datasets/Caltech256/">Caltech-256</a> and
<a class="reference external" href="http://image-net.org/">ImageNet-1K</a> .</p>
<p>They share 84 common classes. ImageNet-Caltech keeps all classes of ImageNet-1K.
The label is based on the ImageNet-1K (class 0-999) . The private classes of Caltech-256 is discarded.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'C'</span></code>:Caltech-256,             <code class="docutils literal notranslate"><span class="pre">'I'</span></code>: ImageNet-1K training set.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to put <code class="docutils literal notranslate"><span class="pre">train</span></code> and <code class="docutils literal notranslate"><span class="pre">val</span></code> directory of ImageNet-1K manually in <cite>root</cite> directory
since ImageNet-1K is no longer publicly accessible. DALIB will only download Caltech-256 and ImageList automatically.
In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
    <span class="n">n01440764</span><span class="o">/</span>
    <span class="o">...</span>
<span class="n">val</span><span class="o">/</span>
<span class="mi">256</span><span class="n">_ObjectCategories</span><span class="o">/</span>
    <span class="mf">001.</span><span class="n">ak47</span><span class="o">/</span>
    <span class="o">...</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">caltech_256_list</span><span class="o">.</span><span class="n">txt</span>
    <span class="o">...</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="open-set-cross-domain-classification">
<h2>Open Set Cross-Domain Classification<a class="headerlink" href="#open-set-cross-domain-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="open-set-wrapper">
<h3>Open Set Wrapper<a class="headerlink" href="#open-set-wrapper" title="Permalink to this headline">¶</a></h3>
<dl class="function">
<dt id="tllib.vision.datasets.openset.open_set">
<code class="sig-prename descclassname">tllib.vision.datasets.openset.</code><code class="sig-name descname">open_set</code><span class="sig-paren">(</span><em class="sig-param">dataset_class</em>, <em class="sig-param">public_classes</em>, <em class="sig-param">private_classes=()</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/openset.html#open_set"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.openset.open_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a dataset into its open-set version.</p>
<p>In other words, those samples which doesn’t belong to <cite>private_classes</cite> will be marked as “unknown”.</p>
<p>Be aware that <cite>open_set</cite> will change the label number of each category.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset_class</strong> (<em>class</em>) – Dataset class. Only subclass of <code class="docutils literal notranslate"><span class="pre">ImageList</span></code> can be open-set.</p></li>
<li><p><strong>public_classes</strong> (<em>sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>]</em>) – A sequence of which categories need to be kept in the open-set dataset.            Each element of <cite>public_classes</cite> must belong to the <cite>classes</cite> list of <cite>dataset_class</cite>.</p></li>
<li><p><strong>private_classes</strong> (<em>sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>]</em><em>, </em><em>optional</em>) – A sequence of which categories need to be marked as “unknown”             in the open-set dataset. Each element of <cite>private_classes</cite> must belong to the <cite>classes</cite> list of             <cite>dataset_class</cite>. Default: ().</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">public_classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;back_pack&#39;</span><span class="p">,</span> <span class="s1">&#39;bike&#39;</span><span class="p">,</span> <span class="s1">&#39;calculator&#39;</span><span class="p">,</span> <span class="s1">&#39;headphones&#39;</span><span class="p">,</span> <span class="s1">&#39;keyboard&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">private_classes</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;laptop_computer&#39;</span><span class="p">,</span> <span class="s1">&#39;monitor&#39;</span><span class="p">,</span> <span class="s1">&#39;mouse&#39;</span><span class="p">,</span> <span class="s1">&#39;mug&#39;</span><span class="p">,</span> <span class="s1">&#39;projector&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># create a open-set dataset class which has classes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># &#39;back_pack&#39;, &#39;bike&#39;, &#39;calculator&#39;, &#39;headphones&#39;, &#39;keyboard&#39; and &#39;unknown&#39;.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">OpenSetOffice31</span> <span class="o">=</span> <span class="n">open_set</span><span class="p">(</span><span class="n">Office31</span><span class="p">,</span> <span class="n">public_classes</span><span class="p">,</span> <span class="n">private_classes</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># create an instance of the open-set dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">OpenSetDataset</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s2">&quot;data/office31&quot;</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;A&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="tllib.vision.datasets.openset.default_open_set">
<code class="sig-prename descclassname">tllib.vision.datasets.openset.</code><code class="sig-name descname">default_open_set</code><span class="sig-paren">(</span><em class="sig-param">dataset_class</em>, <em class="sig-param">source</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/openset.html#default_open_set"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.openset.default_open_set" title="Permalink to this definition">¶</a></dt>
<dd><p>Default open-set used in some paper.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataset_class</strong> (<em>class</em>) – Dataset class. Currently, dataset_class must be one of
<a class="reference internal" href="#tllib.vision.datasets.office31.Office31" title="tllib.vision.datasets.office31.Office31"><code class="xref py py-class docutils literal notranslate"><span class="pre">Office31</span></code></a>, <a class="reference internal" href="#tllib.vision.datasets.officehome.OfficeHome" title="tllib.vision.datasets.officehome.OfficeHome"><code class="xref py py-class docutils literal notranslate"><span class="pre">OfficeHome</span></code></a>,
<a class="reference internal" href="#tllib.vision.datasets.visda2017.VisDA2017" title="tllib.vision.datasets.visda2017.VisDA2017"><code class="xref py py-class docutils literal notranslate"><span class="pre">VisDA2017</span></code></a>,</p></li>
<li><p><strong>source</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a>) – Whether the dataset is used for source domain or not.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="cross-domain-regression">
<h2>Cross-Domain Regression<a class="headerlink" href="#cross-domain-regression" title="Permalink to this headline">¶</a></h2>
<div class="section" id="imageregression">
<h3>ImageRegression<a class="headerlink" href="#imageregression" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.regression.image_regression.ImageRegression">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.regression.image_regression.</code><code class="sig-name descname">ImageRegression</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">factors</em>, <em class="sig-param">data_list_file</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">target_transform=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/regression/image_regression.html#ImageRegression"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.regression.image_regression.ImageRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>A generic Dataset class for domain adaptation in image regression</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>factors</strong> (<em>sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>]</em>) – Factors selected. Default: (‘scale’, ‘position x’, ‘position y’).</p></li>
<li><p><strong>data_list_file</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – File to read the image list from.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>data_list_file</cite>, each line has <cite>1+len(factors)</cite> values in the following format.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source_dir</span><span class="o">/</span><span class="n">dog_xxx</span><span class="o">.</span><span class="n">png</span> <span class="n">x11</span><span class="p">,</span> <span class="n">x12</span><span class="p">,</span> <span class="o">...</span>
<span class="n">source_dir</span><span class="o">/</span><span class="n">cat_123</span><span class="o">.</span><span class="n">png</span> <span class="n">x21</span><span class="p">,</span> <span class="n">x22</span><span class="p">,</span> <span class="o">...</span>
<span class="n">target_dir</span><span class="o">/</span><span class="n">dog_xxy</span><span class="o">.</span><span class="n">png</span> <span class="n">x31</span><span class="p">,</span> <span class="n">x32</span><span class="p">,</span> <span class="o">...</span>
<span class="n">target_dir</span><span class="o">/</span><span class="n">cat_nsdf3</span><span class="o">.</span><span class="n">png</span> <span class="n">x41</span><span class="p">,</span> <span class="n">x42</span><span class="p">,</span> <span class="o">...</span>
</pre></div>
</div>
<p>The first value is the relative path of an image, and the rest values are the ground truth of the corresponding factors.
If your data_list_file has different formats, please over-ride <a class="reference internal" href="#tllib.vision.datasets.regression.image_regression.ImageRegression.parse_data_file" title="tllib.vision.datasets.regression.image_regression.ImageRegression.parse_data_file"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ImageRegression.parse_data_file()</span></code></a>.</p>
</div>
<dl class="method">
<dt id="tllib.vision.datasets.regression.image_regression.ImageRegression.parse_data_file">
<code class="sig-name descname">parse_data_file</code><span class="sig-paren">(</span><em class="sig-param">file_name</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/regression/image_regression.html#ImageRegression.parse_data_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.regression.image_regression.ImageRegression.parse_data_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse file to data list</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>file_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The path of data file</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of (image path, (factors)) tuples</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dsprites">
<h3>DSprites<a class="headerlink" href="#dsprites" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.regression.dsprites.DSprites">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.regression.dsprites.</code><code class="sig-name descname">DSprites</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">factors=('scale'</em>, <em class="sig-param">'position x'</em>, <em class="sig-param">'position y')</em>, <em class="sig-param">download=True</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/regression/dsprites.html#DSprites"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.regression.dsprites.DSprites" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://github.com/deepmind/dsprites-dataset">DSprites</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'C'</span></code>: Color,             <code class="docutils literal notranslate"><span class="pre">'N'</span></code>: Noisy and <code class="docutils literal notranslate"><span class="pre">'S'</span></code>: Scream.</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>factors</strong> (<em>sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>]</em>) – Factors selected. Default: (‘scale’, ‘position x’, ‘position y’).</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">color</span><span class="o">/</span>
    <span class="o">...</span>
<span class="n">noisy</span><span class="o">/</span>
<span class="n">scream</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">color_train</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">noisy_train</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">scream_train</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">color_test</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">noisy_test</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">scream_test</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="mpi3d">
<h3>MPI3D<a class="headerlink" href="#mpi3d" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.regression.mpi3d.MPI3D">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.regression.mpi3d.</code><code class="sig-name descname">MPI3D</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">task</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">factors=('horizontal axis'</em>, <em class="sig-param">'vertical axis')</em>, <em class="sig-param">download=True</em>, <em class="sig-param">target_transform=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/regression/mpi3d.html#MPI3D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.regression.mpi3d.MPI3D" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://arxiv.org/abs/1906.03292">MPI3D</a> Dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The task (domain) to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'C'</span></code>: Color,             <code class="docutils literal notranslate"><span class="pre">'N'</span></code>: Noisy and <code class="docutils literal notranslate"><span class="pre">'S'</span></code>: Scream.</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>factors</strong> (<em>sequence</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>]</em>) – Factors selected. Default: (‘horizontal axis’, ‘vertical axis’).</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">real</span><span class="o">/</span>
    <span class="o">...</span>
<span class="n">realistic</span><span class="o">/</span>
<span class="n">toy</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">real_train</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">realistic_train</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">toy_train</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">real_test</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">realistic_test</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">toy_test</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="cross-domain-segmentation">
<h2>Cross-Domain Segmentation<a class="headerlink" href="#cross-domain-segmentation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="segmentationlist">
<h3>SegmentationList<a class="headerlink" href="#segmentationlist" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.segmentation.segmentation_list.</code><code class="sig-name descname">SegmentationList</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">classes</em>, <em class="sig-param">data_list_file</em>, <em class="sig-param">label_list_file</em>, <em class="sig-param">data_folder</em>, <em class="sig-param">label_folder</em>, <em class="sig-param">id_to_train_id=None</em>, <em class="sig-param">train_id_to_color=None</em>, <em class="sig-param">transforms=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/segmentation_list.html#SegmentationList"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList" title="Permalink to this definition">¶</a></dt>
<dd><p>A generic Dataset class for domain adaptation in image segmentation</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>classes</strong> (<em>seq</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>]</em>) – The names of all the classes</p></li>
<li><p><strong>data_list_file</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – File to read the image list from.</p></li>
<li><p><strong>label_list_file</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – File to read the label list from.</p></li>
<li><p><strong>data_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Sub-directory of the image.</p></li>
<li><p><strong>label_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Sub-directory of the label.</p></li>
<li><p><strong>mean</strong> (<em>seq</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>]</em>) – mean BGR value. Normalize and convert to the image if not None. Default: None.</p></li>
<li><p><strong>id_to_train_id</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)"><em>dict</em></a><em>, </em><em>optional</em>) – the map between the id on the label and the actual train id.</p></li>
<li><p><strong>train_id_to_color</strong> (<em>seq</em><em>, </em><em>optional</em>) – the map between the train id and the color.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in  (PIL Image, label) pair             and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.segmentation.Resize" title="tllib.vision.transforms.segmentation.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <code class="docutils literal notranslate"><span class="pre">data_list_file</span></code>, each line is the relative path of an image.
If your data_list_file has different formats, please over-ride <a class="reference internal" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.parse_data_file" title="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.parse_data_file"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse_data_file()</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source_dir</span><span class="o">/</span><span class="n">dog_xxx</span><span class="o">.</span><span class="n">png</span>
<span class="n">target_dir</span><span class="o">/</span><span class="n">dog_xxy</span><span class="o">.</span><span class="n">png</span>
</pre></div>
</div>
<p>In <code class="docutils literal notranslate"><span class="pre">label_list_file</span></code>, each line is the relative path of an label.
If your label_list_file has different formats, please over-ride <a class="reference internal" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.parse_label_file" title="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.parse_label_file"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse_label_file()</span></code></a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>When mean is not None, please do not provide Normalize and ToTensor in transforms.</p>
</div>
<dl class="method">
<dt id="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.collect_image_paths">
<code class="sig-name descname">collect_image_paths</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/segmentation_list.html#SegmentationList.collect_image_paths"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.collect_image_paths" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of the absolute path of all the images</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.decode_target">
<code class="sig-name descname">decode_target</code><span class="sig-paren">(</span><em class="sig-param">target</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/segmentation_list.html#SegmentationList.decode_target"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.decode_target" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode label (each value is integer) into the corresponding RGB value.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>target</strong> (<em>numpy.array</em>) – label in shape H x W</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>RGB label (PIL Image) in shape H x W x 3</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.evaluate_classes">
<em class="property">property </em><code class="sig-name descname">evaluate_classes</code><a class="headerlink" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.evaluate_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>The name of classes to be evaluated</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.ignore_classes">
<em class="property">property </em><code class="sig-name descname">ignore_classes</code><a class="headerlink" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.ignore_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>The name of classes to be ignored</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.num_classes">
<em class="property">property </em><code class="sig-name descname">num_classes</code><a class="headerlink" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.num_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of classes</p>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.parse_data_file">
<code class="sig-name descname">parse_data_file</code><span class="sig-paren">(</span><em class="sig-param">file_name</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/segmentation_list.html#SegmentationList.parse_data_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.parse_data_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse file to image list</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>file_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The path of data file</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of image path</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.parse_label_file">
<code class="sig-name descname">parse_label_file</code><span class="sig-paren">(</span><em class="sig-param">file_name</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/segmentation_list.html#SegmentationList.parse_label_file"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.parse_label_file" title="Permalink to this definition">¶</a></dt>
<dd><p>Parse file to label list</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>file_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – The path of data file</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>List of label path</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.translate">
<code class="sig-name descname">translate</code><span class="sig-paren">(</span><em class="sig-param">transform</em>, <em class="sig-param">target_root</em>, <em class="sig-param">color=False</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/segmentation_list.html#SegmentationList.translate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.segmentation_list.SegmentationList.translate" title="Permalink to this definition">¶</a></dt>
<dd><p>Translate an image and save it into a specified directory</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>transform</strong> (<em>callable</em>) – a transform function that maps (image, label) pair from one domain to another domain</p></li>
<li><p><strong>target_root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – the root directory to save images and labels</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="cityscapes">
<h3>Cityscapes<a class="headerlink" href="#cityscapes" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.segmentation.cityscapes.Cityscapes">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.segmentation.cityscapes.</code><code class="sig-name descname">Cityscapes</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">data_folder='leftImg8bit'</em>, <em class="sig-param">label_folder='gtFine'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/cityscapes.html#Cityscapes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.cityscapes.Cityscapes" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.cityscapes-dataset.com/">Cityscapes</a> is a real-world semantic segmentation dataset collected
in driving scenarios.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">val</span></code>.</p></li>
<li><p><strong>data_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Sub-directory of the image. Default: ‘leftImg8bit’.</p></li>
<li><p><strong>label_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Sub-directory of the label. Default: ‘gtFine’.</p></li>
<li><p><strong>mean</strong> (<em>seq</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>]</em>) – mean BGR value. Normalize the image if not None. Default: None.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in  (PIL image, label) pair             and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.segmentation.Resize" title="tllib.vision.transforms.segmentation.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to download Cityscapes manually.
Ensure that there exist following files in the <cite>root</cite> directory before you using this class.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">leftImg8bit</span><span class="o">/</span>
    <span class="n">train</span><span class="o">/</span>
    <span class="n">val</span><span class="o">/</span>
    <span class="n">test</span><span class="o">/</span>
<span class="n">gtFine</span><span class="o">/</span>
    <span class="n">train</span><span class="o">/</span>
    <span class="n">val</span><span class="o">/</span>
    <span class="n">test</span><span class="o">/</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="gta5">
<h3>GTA5<a class="headerlink" href="#gta5" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.segmentation.gta5.GTA5">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.segmentation.gta5.</code><code class="sig-name descname">GTA5</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">data_folder='images'</em>, <em class="sig-param">label_folder='labels'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/gta5.html#GTA5"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.gta5.GTA5" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://download.visinf.tu-darmstadt.de/data/from_games/">GTA5</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>.</p></li>
<li><p><strong>data_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Sub-directory of the image. Default: ‘images’.</p></li>
<li><p><strong>label_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Sub-directory of the label. Default: ‘labels’.</p></li>
<li><p><strong>mean</strong> (<em>seq</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>]</em>) – mean BGR value. Normalize the image if not None. Default: None.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in  (PIL image, label) pair             and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.segmentation.Resize" title="tllib.vision.transforms.segmentation.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to download GTA5 manually.
Ensure that there exist following directories in the <cite>root</cite> directory before you using this class.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">images</span><span class="o">/</span>
<span class="n">labels</span><span class="o">/</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="synthia">
<h3>Synthia<a class="headerlink" href="#synthia" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.segmentation.synthia.Synthia">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.segmentation.synthia.</code><code class="sig-name descname">Synthia</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">data_folder='RGB'</em>, <em class="sig-param">label_folder='synthia_mapped_to_cityscapes'</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/synthia.html#Synthia"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.synthia.Synthia" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://synthia-dataset.net/">SYNTHIA</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>.</p></li>
<li><p><strong>data_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Sub-directory of the image. Default: ‘RGB’.</p></li>
<li><p><strong>label_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Sub-directory of the label. Default: ‘synthia_mapped_to_cityscapes’.</p></li>
<li><p><strong>mean</strong> (<em>seq</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>]</em>) – mean BGR value. Normalize the image if not None. Default: None.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in  (PIL image, label) pair             and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.segmentation.Resize" title="tllib.vision.transforms.segmentation.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to download GTA5 manually.
Ensure that there exist following directories in the <cite>root</cite> directory before you using this class.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">RGB</span><span class="o">/</span>
<span class="n">synthia_mapped_to_cityscapes</span><span class="o">/</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="foggy-cityscapes">
<h3>Foggy Cityscapes<a class="headerlink" href="#foggy-cityscapes" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.segmentation.cityscapes.FoggyCityscapes">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.segmentation.cityscapes.</code><code class="sig-name descname">FoggyCityscapes</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">data_folder='leftImg8bit_foggy'</em>, <em class="sig-param">label_folder='gtFine'</em>, <em class="sig-param">beta=0.02</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/segmentation/cityscapes.html#FoggyCityscapes"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.segmentation.cityscapes.FoggyCityscapes" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.cityscapes-dataset.com/">Foggy Cityscapes</a> is a real-world semantic segmentation dataset collected
in foggy driving scenarios.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">val</span></code>.</p></li>
<li><p><strong>data_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Sub-directory of the image. Default: ‘leftImg8bit’.</p></li>
<li><p><strong>label_folder</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Sub-directory of the label. Default: ‘gtFine’.</p></li>
<li><p><strong>beta</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>, </em><em>optional</em>) – The parameter for foggy. Choices includes: 0.005, 0.01, 0.02. Default: 0.02</p></li>
<li><p><strong>mean</strong> (<em>seq</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.10)"><em>float</em></a><em>]</em>) – mean BGR value. Normalize the image if not None. Default: None.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in  (PIL image, label) pair             and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.segmentation.Resize" title="tllib.vision.transforms.segmentation.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to download Cityscapes manually.
Ensure that there exist following files in the <cite>root</cite> directory before you using this class.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">leftImg8bit_foggy</span><span class="o">/</span>
    <span class="n">train</span><span class="o">/</span>
    <span class="n">val</span><span class="o">/</span>
    <span class="n">test</span><span class="o">/</span>
<span class="n">gtFine</span><span class="o">/</span>
    <span class="n">train</span><span class="o">/</span>
    <span class="n">val</span><span class="o">/</span>
    <span class="n">test</span><span class="o">/</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="cross-domain-keypoint-detection">
<h2>Cross-Domain Keypoint Detection<a class="headerlink" href="#cross-domain-keypoint-detection" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dataset-base-for-keypoint-detection">
<h3>Dataset Base for Keypoint Detection<a class="headerlink" href="#dataset-base-for-keypoint-detection" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.keypoint_detection.keypoint_dataset.KeypointDataset">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.keypoint_detection.keypoint_dataset.</code><code class="sig-name descname">KeypointDataset</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">num_keypoints</em>, <em class="sig-param">samples</em>, <em class="sig-param">transforms=None</em>, <em class="sig-param">image_size=(256</em>, <em class="sig-param">256)</em>, <em class="sig-param">heatmap_size=(64</em>, <em class="sig-param">64)</em>, <em class="sig-param">sigma=2</em>, <em class="sig-param">keypoints_group=None</em>, <em class="sig-param">colored_skeleton=None</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/keypoint_dataset.html#KeypointDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.keypoint_dataset.KeypointDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>A generic dataset class for image keypoint detection</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>num_keypoints</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – Number of keypoints</p></li>
<li><p><strong>samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – list of data</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a dict (which contains PIL image and
its labels) and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.keypoint_detection.Resize" title="tllib.vision.transforms.keypoint_detection.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
<li><p><strong>image_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the image. Default: (256, 256)</p></li>
<li><p><strong>heatmap_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the heatmap. Default: (64, 64)</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – sigma parameter when generate the heatmap. Default: 2</p></li>
<li><p><strong>keypoints_group</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)"><em>dict</em></a>) – a dict that stores the index of different types of keypoints</p></li>
<li><p><strong>colored_skeleton</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.10)"><em>dict</em></a>) – a dict that stores the index and color of different skeleton</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="tllib.vision.datasets.keypoint_detection.keypoint_dataset.KeypointDataset.group_accuracy">
<code class="sig-name descname">group_accuracy</code><span class="sig-paren">(</span><em class="sig-param">accuracies</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/keypoint_dataset.html#KeypointDataset.group_accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.keypoint_dataset.KeypointDataset.group_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Group the accuracy of K keypoints into different kinds.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>accuracies</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.10)"><em>list</em></a>) – accuracy of the K keypoints</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>accuracy of <code class="docutils literal notranslate"><span class="pre">N=len(keypoints_group)</span></code> kinds of keypoints</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="tllib.vision.datasets.keypoint_detection.keypoint_dataset.KeypointDataset.visualize">
<code class="sig-name descname">visualize</code><span class="sig-paren">(</span><em class="sig-param">image</em>, <em class="sig-param">keypoints</em>, <em class="sig-param">filename</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/keypoint_dataset.html#KeypointDataset.visualize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.keypoint_dataset.KeypointDataset.visualize" title="Permalink to this definition">¶</a></dt>
<dd><p>Visualize an image with its keypoints, and store the result into a file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>image</strong> (<em>PIL.Image</em>) – </p></li>
<li><p><strong>keypoints</strong> (<a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v1.12)"><em>torch.Tensor</em></a>) – keypoints in shape K x 2</p></li>
<li><p><strong>filename</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – the name of file to store</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="tllib.vision.datasets.keypoint_detection.keypoint_dataset.Body16KeypointDataset">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.keypoint_detection.keypoint_dataset.</code><code class="sig-name descname">Body16KeypointDataset</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">samples</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/keypoint_dataset.html#Body16KeypointDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.keypoint_dataset.Body16KeypointDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Dataset with 16 body keypoints.</p>
</dd></dl>

<dl class="class">
<dt id="tllib.vision.datasets.keypoint_detection.keypoint_dataset.Hand21KeypointDataset">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.keypoint_detection.keypoint_dataset.</code><code class="sig-name descname">Hand21KeypointDataset</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">samples</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/keypoint_dataset.html#Hand21KeypointDataset"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.keypoint_dataset.Hand21KeypointDataset" title="Permalink to this definition">¶</a></dt>
<dd><p>Dataset with 21 hand keypoints.</p>
</dd></dl>

</div>
<div class="section" id="rendered-handpose-dataset">
<h3>Rendered Handpose Dataset<a class="headerlink" href="#rendered-handpose-dataset" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.keypoint_detection.rendered_hand_pose.RenderedHandPose">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.keypoint_detection.rendered_hand_pose.</code><code class="sig-name descname">RenderedHandPose</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">task='all'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/rendered_hand_pose.html#RenderedHandPose"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.rendered_hand_pose.RenderedHandPose" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://lmb.informatik.uni-freiburg.de/resources/datasets/RenderedHandposeDataset.en.html">Rendered Handpose Dataset</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">test</span></code>, or <code class="docutils literal notranslate"><span class="pre">all</span></code>.</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Placeholder.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a dict (which contains PIL image and
its labels) and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.keypoint_detection.Resize" title="tllib.vision.transforms.keypoint_detection.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
<li><p><strong>image_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the image. Default: (256, 256)</p></li>
<li><p><strong>heatmap_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the heatmap. Default: (64, 64)</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – sigma parameter when generate the heatmap. Default: 2</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">RHD_published_v2</span><span class="o">/</span>
    <span class="n">training</span><span class="o">/</span>
    <span class="n">evaluation</span><span class="o">/</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="hand-3d-studio-dataset">
<h3>Hand-3d-Studio Dataset<a class="headerlink" href="#hand-3d-studio-dataset" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.keypoint_detection.hand_3d_studio.Hand3DStudio">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.keypoint_detection.hand_3d_studio.</code><code class="sig-name descname">Hand3DStudio</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">task='noobject'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/hand_3d_studio.html#Hand3DStudio"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.hand_3d_studio.Hand3DStudio" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.yangangwang.com/papers/ZHAO-H3S-2020-02.html">Hand-3d-Studio Dataset</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">test</span></code>, or <code class="docutils literal notranslate"><span class="pre">all</span></code>.</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The task to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'noobject'</span></code>: only hands without objects,             <code class="docutils literal notranslate"><span class="pre">'object'</span></code>: only hands interacting with hands, and <code class="docutils literal notranslate"><span class="pre">'all'</span></code>: all hands. Default: ‘noobject’.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a dict (which contains PIL image and
its labels) and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.keypoint_detection.Resize" title="tllib.vision.transforms.keypoint_detection.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
<li><p><strong>image_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the image. Default: (256, 256)</p></li>
<li><p><strong>heatmap_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the heatmap. Default: (64, 64)</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – sigma parameter when generate the heatmap. Default: 2</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We found that the original H3D image is in high resolution while most part in an image is background,
thus we crop the image and keep only the surrounding area of hands (1.5x bigger than hands) to speed up training.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">H3D_crop</span><span class="o">/</span>
    <span class="n">annotation</span><span class="o">.</span><span class="n">json</span>
    <span class="n">part1</span><span class="o">/</span>
    <span class="n">part2</span><span class="o">/</span>
    <span class="n">part3</span><span class="o">/</span>
    <span class="n">part4</span><span class="o">/</span>
    <span class="n">part5</span><span class="o">/</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="freihand-dataset">
<h3>FreiHAND Dataset<a class="headerlink" href="#freihand-dataset" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.keypoint_detection.freihand.FreiHand">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.keypoint_detection.freihand.</code><code class="sig-name descname">FreiHand</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">task='all'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/freihand.html#FreiHand"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.freihand.FreiHand" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://lmb.informatik.uni-freiburg.de/projects/freihand/">FreiHand Dataset</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">test</span></code>, or <code class="docutils literal notranslate"><span class="pre">all</span></code>.</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The post-processing option to create dataset. Choices include <code class="docutils literal notranslate"><span class="pre">'gs'</span></code>: green screen             recording, <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>: auto colorization without sample points: automatic color hallucination,             <code class="docutils literal notranslate"><span class="pre">'sample'</span></code>: auto colorization with sample points, <code class="docutils literal notranslate"><span class="pre">'hom'</span></code>: homogenized,             and <code class="docutils literal notranslate"><span class="pre">'all'</span></code>: all hands. Default: ‘all’.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a dict (which contains PIL image and
its labels) and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.keypoint_detection.Resize" title="tllib.vision.transforms.keypoint_detection.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
<li><p><strong>image_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the image. Default: (256, 256)</p></li>
<li><p><strong>heatmap_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the heatmap. Default: (64, 64)</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – sigma parameter when generate the heatmap. Default: 2</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">*.</span><span class="n">json</span>
<span class="n">training</span><span class="o">/</span>
<span class="n">evaluation</span><span class="o">/</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="surreal-dataset">
<h3>Surreal Dataset<a class="headerlink" href="#surreal-dataset" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.keypoint_detection.surreal.SURREAL">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.keypoint_detection.surreal.</code><code class="sig-name descname">SURREAL</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">task='all'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/surreal.html#SURREAL"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.surreal.SURREAL" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.di.ens.fr/willow/research/surreal/data/">Surreal Dataset</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">test</span></code>, or <code class="docutils literal notranslate"><span class="pre">all</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">train</span></code>.</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Placeholder.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a dict (which contains PIL image and
its labels) and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.keypoint_detection.Resize" title="tllib.vision.transforms.keypoint_detection.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
<li><p><strong>image_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the image. Default: (256, 256)</p></li>
<li><p><strong>heatmap_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the heatmap. Default: (64, 64)</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – sigma parameter when generate the heatmap. Default: 2</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We found that the original Surreal image is in high resolution while most part in an image is background,
thus we crop the image and keep only the surrounding area of hands (1.5x bigger than hands) to speed up training.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
<span class="n">test</span><span class="o">/</span>
<span class="n">val</span><span class="o">/</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="lsp-dataset">
<h3>LSP Dataset<a class="headerlink" href="#lsp-dataset" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.keypoint_detection.lsp.LSP">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.keypoint_detection.lsp.</code><code class="sig-name descname">LSP</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">task='all'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">image_size=(256</em>, <em class="sig-param">256)</em>, <em class="sig-param">transforms=None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/lsp.html#LSP"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.lsp.LSP" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://sam.johnson.io/research/lsp.html">Leeds Sports Pose Dataset</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – PlaceHolder.</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Placeholder.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – PlaceHolder.</p></li>
<li><p><strong>heatmap_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the heatmap. Default: (64, 64)</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – sigma parameter when generate the heatmap. Default: 2</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lsp</span><span class="o">/</span>
    <span class="n">images</span><span class="o">/</span>
    <span class="n">joints</span><span class="o">.</span><span class="n">mat</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>LSP is only used for target domain. Due to the small dataset size, the whole dataset is used
no matter what <code class="docutils literal notranslate"><span class="pre">split</span></code> is. Also, the transform is fixed.</p>
</div>
</dd></dl>

</div>
<div class="section" id="human3-6m-dataset">
<h3>Human3.6M Dataset<a class="headerlink" href="#human3-6m-dataset" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.keypoint_detection.human36m.Human36M">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.keypoint_detection.human36m.</code><code class="sig-name descname">Human36M</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">task='all'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/keypoint_detection/human36m.html#Human36M"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.keypoint_detection.human36m.Human36M" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://vision.imar.ro/human3.6m/description.php">Human3.6M Dataset</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, <code class="docutils literal notranslate"><span class="pre">test</span></code>, or <code class="docutils literal notranslate"><span class="pre">all</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">train</span></code>.</p></li>
<li><p><strong>task</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – Placeholder.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – Placeholder.</p></li>
<li><p><strong>transforms</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in a dict (which contains PIL image and
its labels) and returns a transformed version. E.g, <a class="reference internal" href="transforms.html#tllib.vision.transforms.keypoint_detection.Resize" title="tllib.vision.transforms.keypoint_detection.Resize"><code class="xref py py-class docutils literal notranslate"><span class="pre">Resize</span></code></a>.</p></li>
<li><p><strong>image_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the image. Default: (256, 256)</p></li>
<li><p><strong>heatmap_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.10)"><em>tuple</em></a>) – (width, height) of the heatmap. Default: (64, 64)</p></li>
<li><p><strong>sigma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – sigma parameter when generate the heatmap. Default: 2</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to download Human36M manually.
Ensure that there exist following files in the <cite>root</cite> directory before you using this class.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">annotations</span><span class="o">/</span>
    <span class="n">Human36M_subject11_joint_3d</span><span class="o">.</span><span class="n">json</span>
    <span class="o">...</span>
<span class="n">images</span><span class="o">/</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We found that the original Human3.6M image is in high resolution while most part in an image is background,
thus we crop the image and keep only the surrounding area of hands (1.5x bigger than hands) to speed up training.
In <cite>root</cite>, there will exist following files after crop.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Human36M_crop</span><span class="o">/</span>
<span class="n">annotations</span><span class="o">/</span>
    <span class="n">keypoints2d_11</span><span class="o">.</span><span class="n">json</span>
    <span class="o">...</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
</div>
<div class="section" id="cross-domain-reid">
<h2>Cross-Domain ReID<a class="headerlink" href="#cross-domain-reid" title="Permalink to this headline">¶</a></h2>
<div class="section" id="market1501">
<h3>Market1501<a class="headerlink" href="#market1501" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.reid.market1501.Market1501">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.reid.market1501.</code><code class="sig-name descname">Market1501</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">verbose=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/reid/market1501.html#Market1501"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.reid.market1501.Market1501" title="Permalink to this definition">¶</a></dt>
<dd><p>Market1501 dataset from <a class="reference external" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410490">Scalable Person Re-identification: A Benchmark (ICCV 2015)</a>.</p>
<dl class="simple">
<dt>Dataset statistics:</dt><dd><ul class="simple">
<li><p>identities: 1501 (+1 for background)</p></li>
<li><p>images: 12936 (train) + 3368 (query) + 15913 (gallery)</p></li>
<li><p>cameras: 6</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, print dataset statistics after loading the dataset. Default: True</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="tllib.vision.datasets.reid.market1501.Market1501.translate">
<code class="sig-name descname">translate</code><span class="sig-paren">(</span><em class="sig-param">transform</em>, <em class="sig-param">target_root</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/reid/market1501.html#Market1501.translate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.reid.market1501.Market1501.translate" title="Permalink to this definition">¶</a></dt>
<dd><p>Translate an image and save it into a specified directory</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>transform</strong> (<em>callable</em>) – a transform function that maps images from one domain to another domain</p></li>
<li><p><strong>target_root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – the root directory to save images</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dukemtmc-reid">
<h3>DukeMTMC-reID<a class="headerlink" href="#dukemtmc-reid" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.reid.dukemtmc.DukeMTMC">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.reid.dukemtmc.</code><code class="sig-name descname">DukeMTMC</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">verbose=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/reid/dukemtmc.html#DukeMTMC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.reid.dukemtmc.DukeMTMC" title="Permalink to this definition">¶</a></dt>
<dd><p>DukeMTMC-reID dataset from <a class="reference external" href="https://arxiv.org/pdf/1609.01775v2.pdf">Performance Measures and a Data Set for Multi-Target, Multi-Camera Tracking
(ECCV 2016)</a>.</p>
<dl class="simple">
<dt>Dataset statistics:</dt><dd><ul class="simple">
<li><p>identities: 1404 (train + query)</p></li>
<li><p>images:16522 (train) + 2228 (query) + 17661 (gallery)</p></li>
<li><p>cameras: 8</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, print dataset statistics after loading the dataset. Default: True</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="tllib.vision.datasets.reid.dukemtmc.DukeMTMC.translate">
<code class="sig-name descname">translate</code><span class="sig-paren">(</span><em class="sig-param">transform</em>, <em class="sig-param">target_root</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/reid/dukemtmc.html#DukeMTMC.translate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.reid.dukemtmc.DukeMTMC.translate" title="Permalink to this definition">¶</a></dt>
<dd><p>Translate an image and save it into a specified directory</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>transform</strong> (<em>callable</em>) – a transform function that maps images from one domain to another domain</p></li>
<li><p><strong>target_root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – the root directory to save images</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="msmt17">
<h3>MSMT17<a class="headerlink" href="#msmt17" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.reid.msmt17.MSMT17">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.reid.msmt17.</code><code class="sig-name descname">MSMT17</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">verbose=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/reid/msmt17.html#MSMT17"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.reid.msmt17.MSMT17" title="Permalink to this definition">¶</a></dt>
<dd><p>MSMT17 dataset from <a class="reference external" href="https://arxiv.org/pdf/1711.08565.pdf">Person Transfer GAN to Bridge Domain Gap for Person Re-Identification (CVPR 2018)</a>.</p>
<dl class="simple">
<dt>Dataset statistics:</dt><dd><ul class="simple">
<li><p>identities: 4101</p></li>
<li><p>images: 32621 (train) + 11659 (query) + 82161 (gallery)</p></li>
<li><p>cameras: 15</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, print dataset statistics after loading the dataset. Default: True</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="tllib.vision.datasets.reid.msmt17.MSMT17.translate">
<code class="sig-name descname">translate</code><span class="sig-paren">(</span><em class="sig-param">transform</em>, <em class="sig-param">target_root</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/reid/msmt17.html#MSMT17.translate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.reid.msmt17.MSMT17.translate" title="Permalink to this definition">¶</a></dt>
<dd><p>Translate an image and save it into a specified directory</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>transform</strong> (<em>callable</em>) – a transform function that maps images from one domain to another domain</p></li>
<li><p><strong>target_root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – the root directory to save images</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="natural-object-recognition">
<h2>Natural Object Recognition<a class="headerlink" href="#natural-object-recognition" title="Permalink to this headline">¶</a></h2>
<div class="section" id="stanford-dogs">
<h3>Stanford Dogs<a class="headerlink" href="#stanford-dogs" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.stanford_dogs.StanfordDogs">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.stanford_dogs.</code><code class="sig-name descname">StanfordDogs</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">sample_rate=100</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/stanford_dogs.html#StanfordDogs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.stanford_dogs.StanfordDogs" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://vision.stanford.edu/aditya86/ImageNetDogs/">The Stanford Dogs</a>     contains 20,580 images of 120 breeds of dogs from around the world.     Each category is composed of exactly 100 training examples and around 72 testing examples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>sample_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The sampling rates to sample random <code class="docutils literal notranslate"><span class="pre">training</span></code> images for each category.
Choices include 100, 50, 30, 15. Default: 100.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
<span class="n">test</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">train_100</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_50</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_30</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_15</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">test</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="stanford-cars">
<h3>Stanford Cars<a class="headerlink" href="#stanford-cars" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.stanford_cars.StanfordCars">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.stanford_cars.</code><code class="sig-name descname">StanfordCars</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">sample_rate=100</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/stanford_cars.html#StanfordCars"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.stanford_cars.StanfordCars" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://ai.stanford.edu/~jkrause/cars/car_dataset.html">The Stanford Cars</a>     contains 16,185 images of 196 classes of cars.     Each category has been split roughly in a 50-50 split.     There are 8,144 images for training and 8,041 images for testing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>sample_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The sampling rates to sample random <code class="docutils literal notranslate"><span class="pre">training</span></code> images for each category.
Choices include 100, 50, 30, 15. Default: 100.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
<span class="n">test</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">train_100</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_50</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_30</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_15</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">test</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="cub-200-2011">
<h3>CUB-200-2011<a class="headerlink" href="#cub-200-2011" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.cub200.CUB200">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.cub200.</code><code class="sig-name descname">CUB200</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">sample_rate=100</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/cub200.html#CUB200"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.cub200.CUB200" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://www.vision.caltech.edu/visipedia/CUB-200-2011.html">Caltech-UCSD Birds-200-2011</a>      is a dataset for fine-grained visual recognition with 11,788 images in 200 bird species.     It is an extended version of the CUB-200 dataset, roughly doubling the number of images.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>sample_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The sampling rates to sample random <code class="docutils literal notranslate"><span class="pre">training</span></code> images for each category.
Choices include 100, 50, 30, 15. Default: 100.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
<span class="n">test</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">train_100</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_50</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_30</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_15</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">test</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="fvgc-aircraft">
<h3>FVGC Aircraft<a class="headerlink" href="#fvgc-aircraft" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.aircrafts.Aircraft">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.aircrafts.</code><code class="sig-name descname">Aircraft</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">sample_rate=100</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/aircrafts.html#Aircraft"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.aircrafts.Aircraft" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/">FVGC-Aircraft</a>         is a benchmark for the fine-grained visual categorization of aircraft.          The dataset contains 10,200 images of aircraft, with 100 images for each         of the 102 different aircraft variants.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>sample_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The sampling rates to sample random <code class="docutils literal notranslate"><span class="pre">training</span></code> images for each category.
Choices include 100, 50, 30, 15. Default: 100.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
<span class="n">test</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">train_100</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_50</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_30</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_15</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">test</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="oxford-iiit-pets">
<h3>Oxford-IIIT Pets<a class="headerlink" href="#oxford-iiit-pets" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.oxfordpets.OxfordIIITPets">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.oxfordpets.</code><code class="sig-name descname">OxfordIIITPets</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">sample_rate=100</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/oxfordpets.html#OxfordIIITPets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.oxfordpets.OxfordIIITPets" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.robots.ox.ac.uk/~vgg/data/pets/">The Oxford-IIIT Pets</a>     is a 37-category pet dataset with roughly 200 images for each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>sample_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The sampling rates to sample random <code class="docutils literal notranslate"><span class="pre">training</span></code> images for each category.
Choices include 100, 50, 30, 15. Default: 100.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
<span class="n">test</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">train_100</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_50</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_30</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_15</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">test</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="coco-70">
<h3>COCO-70<a class="headerlink" href="#coco-70" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.coco70.COCO70">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.coco70.</code><code class="sig-name descname">COCO70</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">sample_rate=100</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/coco70.html#COCO70"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.coco70.COCO70" title="Permalink to this definition">¶</a></dt>
<dd><p>COCO-70 dataset is a large-scale classification dataset (1000 images per class) created from
<a class="reference external" href="https://cocodataset.org/">COCO</a> Dataset.
It is used to explore the effect of fine-tuning with a large amount of data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>sample_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.10)"><em>int</em></a>) – The sampling rates to sample random <code class="docutils literal notranslate"><span class="pre">training</span></code> images for each category.
Choices include 100, 50, 30, 15. Default: 100.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
<span class="n">test</span><span class="o">/</span>
<span class="n">image_list</span><span class="o">/</span>
    <span class="n">train_100</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_50</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_30</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">train_15</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">test</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="dtd">
<h3>DTD<a class="headerlink" href="#dtd" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.dtd.DTD">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.dtd.</code><code class="sig-name descname">DTD</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/dtd.html#DTD"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.dtd.DTD" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.robots.ox.ac.uk/~vgg/data/dtd/index.html">The Describable Textures Dataset (DTD)</a> is an         evolving collection of textural images in the wild, annotated with a series of human-centric attributes,          inspired by the perceptual properties of textures.          The task consists in classifying images of textural patterns (47 classes, with 120 training images each).          Some of the textures are banded, bubbly, meshed, lined, or porous.          The image size ranges between 300x300 and 640x640 pixels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="oxfordflowers102">
<h3>OxfordFlowers102<a class="headerlink" href="#oxfordflowers102" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.oxfordflowers.OxfordFlowers102">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.oxfordflowers.</code><code class="sig-name descname">OxfordFlowers102</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/oxfordflowers.html#OxfordFlowers102"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.oxfordflowers.OxfordFlowers102" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.robots.ox.ac.uk/~vgg/data/flowers/102/">The Oxford Flowers 102</a> is a          consistent of 102 flower categories commonly occurring in the United Kingdom.          Each class consists of between 40 and 258 images. The images have large scale,          pose and light variations. In addition, there are categories that have large          variations within the category and several very similar categories.          The dataset is divided into a training set, a validation set and a test set.          The training set and validation set each consist of 10 images per class          (totalling 1020 images each).          The test set consists of the remaining 6149 images (minimum 20 per class).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="caltech101">
<h3>Caltech101<a class="headerlink" href="#caltech101" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.caltech101.Caltech101">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.caltech101.</code><code class="sig-name descname">Caltech101</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/caltech101.html#Caltech101"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.caltech101.Caltech101" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">The Caltech101 Dataset</a> contains objects
belonging to 101 categories with about 40 to 800 images per category. Most categories have about 50 images.
The size of each image is roughly 300 x 200 pixels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="specialized-image-classification">
<h2>Specialized Image Classification<a class="headerlink" href="#specialized-image-classification" title="Permalink to this headline">¶</a></h2>
<div class="section" id="patchcamelyon">
<h3>PatchCamelyon<a class="headerlink" href="#patchcamelyon" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.patchcamelyon.PatchCamelyon">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.patchcamelyon.</code><code class="sig-name descname">PatchCamelyon</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/patchcamelyon.html#PatchCamelyon"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.patchcamelyon.PatchCamelyon" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference external" href="https://patchcamelyon.grand-challenge.org/">PatchCamelyon</a> dataset contains         327680 images of histopathologic scans of lymph node sections.         The classification task consists in predicting the presence of metastatic tissue          in given image (i.e., two classes). All images are 96x96 pixels</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="retinopathy">
<h3>Retinopathy<a class="headerlink" href="#retinopathy" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.retinopathy.Retinopathy">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.retinopathy.</code><code class="sig-name descname">Retinopathy</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/retinopathy.html#Retinopathy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.retinopathy.Retinopathy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.kaggle.com/c/diabetic-retinopathy-detection/data">Retinopathy</a> dataset         consists of image-label pairs with high-resolution retina images, and labels that indicate         the presence of Diabetic Retinopahy (DR) in a 0-4 scale (No DR, Mild, Moderate, Severe,         or Proliferative DR).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to download the source data manually into <cite>root</cite> directory.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="eurosat">
<h3>EuroSAT<a class="headerlink" href="#eurosat" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.eurosat.EuroSAT">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.eurosat.</code><code class="sig-name descname">EuroSAT</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/eurosat.html#EuroSAT"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.eurosat.EuroSAT" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://github.com/phelber/eurosat">EuroSAT</a> dataset consists in classifying         Sentinel-2 satellite images into 10 different types of land use (Residential,         Industrial, River, Highway, etc).         The spatial resolution corresponds to 10 meters per pixel, and the image size         is 64x64 pixels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="resisc45">
<h3>Resisc45<a class="headerlink" href="#resisc45" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.resisc45.Resisc45">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.resisc45.</code><code class="sig-name descname">Resisc45</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">download=False</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/resisc45.html#Resisc45"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.resisc45.Resisc45" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="http://www.escience.cn/people/JunweiHan/NWPU-RESISC45.html">Resisc45</a> dataset         is a scene classification task from remote sensing images. There are 45 classes,         containing 700 images each, including tennis court, ship, island, lake,         parking lot, sparse residential, or stadium.         The image size is RGB 256x256 pixels.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You need to download the source data manually into <cite>root</cite> directory.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="tllib.vision.datasets.resisc45.Resisc45.num_classes">
<em class="property">property </em><code class="sig-name descname">num_classes</code><a class="headerlink" href="#tllib.vision.datasets.resisc45.Resisc45.num_classes" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of classes</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="food-101">
<h3>Food-101<a class="headerlink" href="#food-101" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.food101.Food101">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.food101.</code><code class="sig-name descname">Food101</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">transform=None</em>, <em class="sig-param">download=True</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/food101.html#Food101"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.food101.Food101" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/">Food-101</a> is a dataset
for fine-grained visual recognition with 101,000 images in 101 food categories.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset.</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In <cite>root</cite>, there will exist following files after downloading.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="o">/</span>
<span class="n">test</span><span class="o">/</span>
</pre></div>
</div>
</div>
</dd></dl>

</div>
<div class="section" id="sun397">
<h3>SUN397<a class="headerlink" href="#sun397" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="tllib.vision.datasets.sun397.SUN397">
<em class="property">class </em><code class="sig-prename descclassname">tllib.vision.datasets.sun397.</code><code class="sig-name descname">SUN397</code><span class="sig-paren">(</span><em class="sig-param">root</em>, <em class="sig-param">split='train'</em>, <em class="sig-param">download=True</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../../_modules/tllib/vision/datasets/sun397.html#SUN397"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#tllib.vision.datasets.sun397.SUN397" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://vision.princeton.edu/projects/2010/SUN/">SUN397</a>  is a dataset for scene understanding
with 108,754 images in 397 scene categories. The number of images varies across categories,
but there are at least 100 images per category. Note that the authors construct 10 partitions,
where each partition contains 50 training images and 50 testing images per class. We adopt partition 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>root</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a>) – Root directory of dataset</p></li>
<li><p><strong>split</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.10)"><em>str</em></a><em>, </em><em>optional</em>) – The dataset split, supports <code class="docutils literal notranslate"><span class="pre">train</span></code>, or <code class="docutils literal notranslate"><span class="pre">test</span></code>.</p></li>
<li><p><strong>download</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.10)"><em>bool</em></a><em>, </em><em>optional</em>) – If true, downloads the dataset from the internet and puts it             in root directory. If dataset is already downloaded, it is not downloaded again.</p></li>
<li><p><strong>transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that  takes in an PIL image and returns a             transformed version. E.g, <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.transforms.RandomCrop.html#torchvision.transforms.RandomCrop" title="(in Torchvision vmain (0.13.1a0+bddbd7e ))"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchvision.transforms.RandomCrop</span></code></a>.</p></li>
<li><p><strong>target_transform</strong> (<em>callable</em><em>, </em><em>optional</em>) – A function/transform that takes in the target and transforms it.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="models.html" class="btn btn-neutral float-right" title="Models" accesskey="n" rel="next">Next <img src="../../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="index.html" class="btn btn-neutral" title="Vision" accesskey="p" rel="prev"><img src="../../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright THUML Group.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://github.com/pytorch/pytorch_sphinx_theme">PyTorch Sphinx Theme</a>.
      </div>
      <!-- <div>
        <a href="https://beian.miit.gov.cn/">京ICP备16023543号-1</a>
      </div> -->
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Datasets</a><ul>
<li><a class="reference internal" href="#cross-domain-classification">Cross-Domain Classification</a><ul>
<li><a class="reference internal" href="#imagelist">ImageList</a></li>
<li><a class="reference internal" href="#office-31">Office-31</a></li>
<li><a class="reference internal" href="#office-caltech">Office-Caltech</a></li>
<li><a class="reference internal" href="#office-home">Office-Home</a></li>
<li><a class="reference internal" href="#visda-2017">VisDA-2017</a></li>
<li><a class="reference internal" href="#domainnet">DomainNet</a></li>
<li><a class="reference internal" href="#pacs">PACS</a></li>
<li><a class="reference internal" href="#mnist">MNIST</a></li>
<li><a class="reference internal" href="#usps">USPS</a></li>
<li><a class="reference internal" href="#svhn">SVHN</a></li>
</ul>
</li>
<li><a class="reference internal" href="#partial-cross-domain-classification">Partial Cross-Domain Classification</a><ul>
<li><a class="reference internal" href="#partial-wrapper">Partial Wrapper</a></li>
<li><a class="reference internal" href="#caltech-256-imagenet-1k">Caltech-256-&gt;ImageNet-1k</a></li>
<li><a class="reference internal" href="#imagenet-1k-caltech-256">ImageNet-1k-&gt;Caltech-256</a></li>
</ul>
</li>
<li><a class="reference internal" href="#open-set-cross-domain-classification">Open Set Cross-Domain Classification</a><ul>
<li><a class="reference internal" href="#open-set-wrapper">Open Set Wrapper</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cross-domain-regression">Cross-Domain Regression</a><ul>
<li><a class="reference internal" href="#imageregression">ImageRegression</a></li>
<li><a class="reference internal" href="#dsprites">DSprites</a></li>
<li><a class="reference internal" href="#mpi3d">MPI3D</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cross-domain-segmentation">Cross-Domain Segmentation</a><ul>
<li><a class="reference internal" href="#segmentationlist">SegmentationList</a></li>
<li><a class="reference internal" href="#cityscapes">Cityscapes</a></li>
<li><a class="reference internal" href="#gta5">GTA5</a></li>
<li><a class="reference internal" href="#synthia">Synthia</a></li>
<li><a class="reference internal" href="#foggy-cityscapes">Foggy Cityscapes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cross-domain-keypoint-detection">Cross-Domain Keypoint Detection</a><ul>
<li><a class="reference internal" href="#dataset-base-for-keypoint-detection">Dataset Base for Keypoint Detection</a></li>
<li><a class="reference internal" href="#rendered-handpose-dataset">Rendered Handpose Dataset</a></li>
<li><a class="reference internal" href="#hand-3d-studio-dataset">Hand-3d-Studio Dataset</a></li>
<li><a class="reference internal" href="#freihand-dataset">FreiHAND Dataset</a></li>
<li><a class="reference internal" href="#surreal-dataset">Surreal Dataset</a></li>
<li><a class="reference internal" href="#lsp-dataset">LSP Dataset</a></li>
<li><a class="reference internal" href="#human3-6m-dataset">Human3.6M Dataset</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cross-domain-reid">Cross-Domain ReID</a><ul>
<li><a class="reference internal" href="#market1501">Market1501</a></li>
<li><a class="reference internal" href="#dukemtmc-reid">DukeMTMC-reID</a></li>
<li><a class="reference internal" href="#msmt17">MSMT17</a></li>
</ul>
</li>
<li><a class="reference internal" href="#natural-object-recognition">Natural Object Recognition</a><ul>
<li><a class="reference internal" href="#stanford-dogs">Stanford Dogs</a></li>
<li><a class="reference internal" href="#stanford-cars">Stanford Cars</a></li>
<li><a class="reference internal" href="#cub-200-2011">CUB-200-2011</a></li>
<li><a class="reference internal" href="#fvgc-aircraft">FVGC Aircraft</a></li>
<li><a class="reference internal" href="#oxford-iiit-pets">Oxford-IIIT Pets</a></li>
<li><a class="reference internal" href="#coco-70">COCO-70</a></li>
<li><a class="reference internal" href="#dtd">DTD</a></li>
<li><a class="reference internal" href="#oxfordflowers102">OxfordFlowers102</a></li>
<li><a class="reference internal" href="#caltech101">Caltech101</a></li>
</ul>
</li>
<li><a class="reference internal" href="#specialized-image-classification">Specialized Image Classification</a><ul>
<li><a class="reference internal" href="#patchcamelyon">PatchCamelyon</a></li>
<li><a class="reference internal" href="#retinopathy">Retinopathy</a></li>
<li><a class="reference internal" href="#eurosat">EuroSAT</a></li>
<li><a class="reference internal" href="#resisc45">Resisc45</a></li>
<li><a class="reference internal" href="#food-101">Food-101</a></li>
<li><a class="reference internal" href="#sun397">SUN397</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/language_data.js"></script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <!-- <div class="col-md-4 text-center">
          <h2>Survey</h2>
          <p>Access comprehensive survey for transfer learning</p>
          <a class="with-right-arrow" href="">View Survey</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Papers</h2>
          <p>Access comprehensive paper list for transfer learning</p>
          <a class="with-right-arrow" href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">View Paper List</a>
        </div> -->

        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive documentation for Transfer Learning Library</p>
          <a class="with-right-arrow" href="transfer.thuml.ai">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get started for Transfer Learning Library</p>
          <a class="with-right-arrow" href="http://microhhh.com/get_started/installing.html">Get Started</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Paper List</h2>
          <p>Get started for transfer learning</p>
          <a class="with-right-arrow" href="https://github.com/thuml/Awesome-Transferability-in-Deep-Learning">View Resources</a>
        </div>
      </div>
    </div>
  </div>



  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="transfer.thuml.ai" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="http://microhhh.com/get_started/installing.html">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="resources-mobile-menu-title">
            Docs
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="transfer.thuml.ai">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/audio/stable/index.html">torchaudio</a>
            </li>

            <li>
              <a href="https://pytorch.org/text/stable/index.html">torchtext</a>
            </li>

            <li>
              <a href="https://pytorch.org/vision/stable/index.html">torchvision</a>
            </li>

            <li>
              <a href="https://pytorch.org/elastic/">TorchElastic</a>
            </li>

            <li>
              <a href="https://pytorch.org/serve/">TorchServe</a>
            </li>

            <li>
              <a href="https://pytorch.org/xla">PyTorch on XLA Devices</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/resources">Developer Resources</a>
            </li>

            <li>
              <a href="https://pytorch.org/hub">Models (Beta)</a>
            </li>

            <li>
              <a href="https://pytorch.org/#community-module">Community</a>
            </li>

            <li>
              <a href="https://discuss.pytorch.org/">Forums</a>
            </li>
          </ul>

          <li>
            <a href="https://github.com/thuml/Transfer-Learning-Library/">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>